{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 03:58:35,224 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2023-12-18 03:58:35,438 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-18 03:58:35,558 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-18 03:58:35,558 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-12-18 03:58:35,559 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-18 03:58:35,560 INFO spark.SparkContext: Submitted application: Application\n",
      "2023-12-18 03:58:35,612 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-12-18 03:58:35,637 INFO resource.ResourceProfile: Limiting resource is cpu\n",
      "2023-12-18 03:58:35,638 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-12-18 03:58:35,749 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-12-18 03:58:35,750 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-12-18 03:58:35,751 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-12-18 03:58:35,751 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-12-18 03:58:35,752 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2023-12-18 03:58:36,152 INFO util.Utils: Successfully started service 'sparkDriver' on port 37681.\n",
      "2023-12-18 03:58:36,199 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-12-18 03:58:36,263 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-12-18 03:58:36,291 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-12-18 03:58:36,292 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-12-18 03:58:36,300 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-12-18 03:58:36,364 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-03537eab-85c1-43f9-b646-a213dde173da\n",
      "2023-12-18 03:58:36,412 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-12-18 03:58:36,451 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-12-18 03:58:36,563 INFO util.log: Logging initialized @4412ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-12-18 03:58:36,770 INFO server.Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.21+9-post-Debian-1deb11u1\n",
      "2023-12-18 03:58:36,824 INFO server.Server: Started @4677ms\n",
      "2023-12-18 03:58:36,904 INFO server.AbstractConnector: Started ServerConnector@2e9fcad2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-12-18 03:58:36,904 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-12-18 03:58:36,950 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a2f66ac{/,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:37,261 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "2023-12-18 03:58:37,387 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 64 ms (0 ms spent in bootstraps)\n",
      "2023-12-18 03:58:37,704 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231218035837-0000\n",
      "2023-12-18 03:58:37,720 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46635.\n",
      "2023-12-18 03:58:37,720 INFO netty.NettyBlockTransferService: Server created on 8e6e1ce1cf59:46635\n",
      "2023-12-18 03:58:37,723 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-12-18 03:58:37,736 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8e6e1ce1cf59, 46635, None)\n",
      "2023-12-18 03:58:37,741 INFO storage.BlockManagerMasterEndpoint: Registering block manager 8e6e1ce1cf59:46635 with 434.4 MiB RAM, BlockManagerId(driver, 8e6e1ce1cf59, 46635, None)\n",
      "2023-12-18 03:58:37,746 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8e6e1ce1cf59, 46635, None)\n",
      "2023-12-18 03:58:37,749 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 8e6e1ce1cf59, 46635, None)\n",
      "2023-12-18 03:58:37,757 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231218035837-0000/0 on worker-20231218035728-172.18.0.5-7000 (172.18.0.5:7000) with 1 core(s)\n",
      "2023-12-18 03:58:37,762 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231218035837-0000/0 on hostPort 172.18.0.5:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-18 03:58:37,764 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231218035837-0000/1 on worker-20231218035728-172.18.0.6-7000 (172.18.0.6:7000) with 1 core(s)\n",
      "2023-12-18 03:58:37,765 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231218035837-0000/1 on hostPort 172.18.0.6:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-18 03:58:38,165 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231218035837-0000/1 is now RUNNING\n",
      "2023-12-18 03:58:38,168 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231218035837-0000/0 is now RUNNING\n",
      "2023-12-18 03:58:39,365 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-12-18 03:58:39,395 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-12-18 03:58:39,396 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-12-18 03:58:42,050 INFO history.SingleEventLogFileWriter: Logging events to s3a://spark-events/app-20231218035837-0000.inprogress\n",
      "2023-12-18 03:58:42,437 WARN s3a.S3ABlockOutputStream: Application invoked the Syncable API against stream writing to app-20231218035837-0000.inprogress. This is Unsupported\n",
      "2023-12-18 03:58:42,483 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4a2f66ac{/,null,STOPPED,@Spark}\n",
      "2023-12-18 03:58:42,485 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7959bf27{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,490 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@711d376a{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,495 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@685baced{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,497 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bdbca4d{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,499 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ebbdf0d{/stages,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,501 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3030765d{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,503 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2148fc0b{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,505 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a29e0d5{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,507 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25439e09{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,509 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a2029df{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,510 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c5eb08b{/storage,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,512 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ad1741c{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,514 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@602abc49{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,516 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5680cb75{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,518 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3649c9a0{/environment,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,520 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@304407d9{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,522 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c03ecf1{/executors,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61a5888b{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,528 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fb8e9b3{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,530 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ace18c4{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,550 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@711a08b6{/static,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,552 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70282ed1{/,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,555 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4469654b{/api,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,557 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f4f7d29{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,559 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77717807{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,571 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56a8c575{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:58:42,573 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "2023-12-18 03:58:42,828 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:37538) with ID 0,  ResourceProfileId 0\n",
      "2023-12-18 03:58:42,830 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:34158) with ID 1,  ResourceProfileId 0\n",
      "2023-12-18 03:58:43,167 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:45633 with 127.2 MiB RAM, BlockManagerId(1, 172.18.0.6, 45633, None)\n",
      "2023-12-18 03:58:43,174 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:38751 with 127.2 MiB RAM, BlockManagerId(0, 172.18.0.5, 38751, None)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"Application\"). \\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.access.key\", f\"{os.environ['MINIO_ROOT_USER']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.secret.key\", f\"{os.environ['MINIO_ROOT_PASSWORD']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{os.environ['MINIO_IP']}:9000\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"s3a://spark-events/\").\\\n",
    "        config(\"spark.history.fs.logDirectory\", \"s3a://spark-events/\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8e6e1ce1cf59:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f424416e6b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 04:00:04,798 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 8e6e1ce1cf59:46635 in memory (size: 53.4 KiB, free: 434.3 MiB)\n",
      "2023-12-18 04:00:04,810 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:45633 in memory (size: 53.4 KiB, free: 127.2 MiB)\n",
      "2023-12-18 04:00:04,828 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 8e6e1ce1cf59:46635 in memory (size: 53.4 KiB, free: 434.4 MiB)\n",
      "2023-12-18 04:00:04,838 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 8e6e1ce1cf59:46635 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-12-18 04:00:04,842 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.6:45633 in memory (size: 5.9 KiB, free: 127.2 MiB)\n",
      "2023-12-18 04:15:49,129 WARN client.StandaloneAppClient$ClientEndpoint: Connection to 172.18.0.4:7077 failed; waiting for master to reconnect...\n",
      "2023-12-18 04:15:49,130 WARN cluster.StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "2023-12-18 04:15:49,920 ERROR scheduler.TaskSchedulerImpl: Lost executor 1 on 172.18.0.6: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "2023-12-18 04:15:49,929 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 0)\n",
      "2023-12-18 04:15:49,930 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.\n",
      "2023-12-18 04:15:49,932 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.18.0.6, 45633, None)\n",
      "2023-12-18 04:15:49,934 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor\n",
      "2023-12-18 04:15:49,936 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 0)\n",
      "2023-12-18 04:15:49,955 ERROR scheduler.TaskSchedulerImpl: Lost executor 0 on 172.18.0.5: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "2023-12-18 04:15:49,955 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 1)\n",
      "2023-12-18 04:15:49,956 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.\n",
      "2023-12-18 04:15:49,957 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 172.18.0.5, 38751, None)\n",
      "2023-12-18 04:15:49,957 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor\n",
      "2023-12-18 04:15:49,957 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 1)\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.eventLog.enabled : true\n",
      "spark.executor.memory : 512m\n",
      "spark.driver.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.app.submitTime : 1702825789634\n",
      "spark.app.name : Application\n",
      "spark.eventLog.dir : s3a://spark-events/\n",
      "spark.driver.host : 15d03baddaa3\n",
      "spark.executor.id : driver\n",
      "spark.app.id : app-20231217150951-0000\n",
      "spark.hadoop.fs.s3a.endpoint : http://172.18.0.2:9000\n",
      "spark.rdd.compress : True\n",
      "spark.executor.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.master : spark://spark-master:7077\n",
      "spark.hadoop.fs.s3a.access.key : minioadmin\n",
      "spark.serializer.objectStreamReset : 100\n",
      "spark.driver.port : 46129\n",
      "spark.submit.pyFiles : \n",
      "spark.app.startTime : 1702825789788\n",
      "spark.submit.deployMode : client\n",
      "spark.history.fs.logDirectory : s3a://spark-events/\n",
      "spark.ui.showConsoleProgress : true\n",
      "spark.sql.warehouse.dir : file:/opt/workspace/code/spark-warehouse\n",
      "spark.hadoop.fs.s3a.secret.key : minioadmin\n"
     ]
    }
   ],
   "source": [
    "conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Print Configuration\n",
    "for k,v in conf:\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-16 13:46:07,726 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-12-16 13:46:07,985 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-12-16 13:46:07,985 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "ls: s3a://datalake/: listObjects() on s3a://datalake/: com.amazonaws.services.s3.model.AmazonS3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: Amazon S3; Status Code: 403; Error Code: InvalidAccessKeyId; Request ID: 8V2MCQF824MC2JJQ; S3 Extended Request ID: QeZZWUYf1Kd63umfn33618/jgONHomRrnsxsPTbw7uXlYsSEVSeNmgyHCGTRt4HRg15sV6G501w=; Proxy: null), S3 Extended Request ID: QeZZWUYf1Kd63umfn33618/jgONHomRrnsxsPTbw7uXlYsSEVSeNmgyHCGTRt4HRg15sV6G501w=:InvalidAccessKeyId\n",
      "2023-12-16 13:46:12,343 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-12-16 13:46:12,348 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-12-16 13:46:12,349 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# !export AWS_ACCESS_KEY_ID=minioadmin\n",
    "# !export AWS_SECRET_ACCESS_KEY=minioadmin\n",
    "# !printenv\n",
    "# !hadoop fs -ls s3a://datalake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 03:59:34,340 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-12-18 03:59:34,345 INFO internal.SharedState: Warehouse path is 'file:/opt/workspace/code/spark-warehouse'.\n",
      "2023-12-18 03:59:34,374 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@145ca698{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:59:34,375 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4806af4d{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:59:34,377 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71d0c46{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:59:34,378 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1739cad2{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:59:34,382 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21d8ce0a{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:59:35,572 INFO datasources.InMemoryFileIndex: It took 90 ms to list leaf files for 1 paths.\n",
      "2023-12-18 03:59:35,695 INFO datasources.InMemoryFileIndex: It took 20 ms to list leaf files for 1 paths.\n",
      "2023-12-18 03:59:38,372 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-18 03:59:38,374 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-12-18 03:59:38,377 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-18 03:59:38,875 INFO codegen.CodeGenerator: Code generated in 153.89949 ms\n",
      "2023-12-18 03:59:38,948 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 302.6 KiB, free 434.1 MiB)\n",
      "2023-12-18 03:59:39,012 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 53.4 KiB, free 434.1 MiB)\n",
      "2023-12-18 03:59:39,016 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 8e6e1ce1cf59:46635 (size: 53.4 KiB, free: 434.3 MiB)\n",
      "2023-12-18 03:59:39,022 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-18 03:59:39,037 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-18 03:59:39,169 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-18 03:59:39,190 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-18 03:59:39,190 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-18 03:59:39,191 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-18 03:59:39,192 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-18 03:59:39,198 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-18 03:59:39,274 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 434.0 MiB)\n",
      "2023-12-18 03:59:39,279 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.0 MiB)\n",
      "2023-12-18 03:59:39,280 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 8e6e1ce1cf59:46635 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "2023-12-18 03:59:39,281 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-18 03:59:39,302 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-18 03:59:39,303 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-12-18 03:59:39,345 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 4917 bytes) taskResourceAssignments Map()\n",
      "2023-12-18 03:59:39,645 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:45633 (size: 5.9 KiB, free: 127.2 MiB)\n",
      "2023-12-18 03:59:40,517 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:45633 (size: 53.4 KiB, free: 127.1 MiB)\n",
      "2023-12-18 03:59:42,918 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3584 ms on 172.18.0.6 (executor 1) (1/1)\n",
      "2023-12-18 03:59:42,922 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-12-18 03:59:42,931 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 3.716 s\n",
      "2023-12-18 03:59:42,936 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-18 03:59:42,937 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-12-18 03:59:42,940 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 3.770726 s\n",
      "2023-12-18 03:59:42,979 INFO codegen.CodeGenerator: Code generated in 19.835214 ms\n",
      "2023-12-18 03:59:43,059 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-18 03:59:43,060 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-18 03:59:43,060 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-18 03:59:43,069 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 302.6 KiB, free 433.7 MiB)\n",
      "2023-12-18 03:59:43,089 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 53.4 KiB, free 433.7 MiB)\n",
      "2023-12-18 03:59:43,091 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 8e6e1ce1cf59:46635 (size: 53.4 KiB, free: 434.3 MiB)\n",
      "2023-12-18 03:59:43,093 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-18 03:59:43,094 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('s3a://datalake/yellow_tripdata_2021-01.csv', header=True)\n",
    "# del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', StringType(), True), StructField('tpep_pickup_datetime', StringType(), True), StructField('tpep_dropoff_datetime', StringType(), True), StructField('passenger_count', StringType(), True), StructField('trip_distance', StringType(), True), StructField('RatecodeID', StringType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('payment_type', StringType(), True), StructField('fare_amount', StringType(), True), StructField('extra', StringType(), True), StructField('mta_tax', StringType(), True), StructField('tip_amount', StringType(), True), StructField('tolls_amount', StringType(), True), StructField('improvement_surcharge', StringType(), True), StructField('total_amount', StringType(), True), StructField('congestion_surcharge', StringType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('VendorID', StringType(), True), StructField('tpep_pickup_datetime', StringType(), True), StructField('tpep_dropoff_datetime', StringType(), True), StructField('passenger_count', StringType(), True), StructField('trip_distance', StringType(), True), StructField('RatecodeID', StringType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('payment_type', StringType(), True), StructField('fare_amount', StringType(), True), StructField('extra', StringType(), True), StructField('mta_tax', StringType(), True), StructField('tip_amount', StringType(), True), StructField('tolls_amount', StringType(), True), StructField('improvement_surcharge', StringType(), True), StructField('total_amount', StringType(), True), StructField('congestion_surcharge', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "     StructField('VendorID', StringType(), True), \n",
    "     StructField('tpep_pickup_datetime', TimestampType(), True), \n",
    "     StructField('tpep_dropoff_datetime', TimestampType(), True), \n",
    "     StructField('passenger_count', IntegerType(), True), \n",
    "     StructField('trip_distance', FloatType(), True), \n",
    "     StructField('RatecodeID', StringType(), True), \n",
    "     StructField('store_and_fwd_flag', StringType(), True), \n",
    "     StructField('PULocationID', StringType(), True), \n",
    "     StructField('DOLocationID', StringType(), True), \n",
    "     StructField('payment_type', StringType(), True), \n",
    "     StructField('fare_amount', FloatType(), True), \n",
    "     StructField('extra', FloatType(), True), \n",
    "     StructField('mta_tax', FloatType(), True), \n",
    "     StructField('tip_amount', FloatType(), True), \n",
    "     StructField('tolls_amount', FloatType(), True), \n",
    "     StructField('improvement_surcharge', FloatType(), True), \n",
    "     StructField('total_amount', FloatType(), True), \n",
    "     StructField('congestion_surcharge', FloatType(), True)\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 15:16:07,463 INFO datasources.InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv('s3a://datalake/yellow_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 15:16:12,487 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 15:16:12,488 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 15:16:12,488 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: int, trip_distance: float ... 16 more fields>\n",
      "2023-12-17 15:16:12,588 INFO codegen.CodeGenerator: Code generated in 40.470843 ms\n",
      "2023-12-17 15:16:12,592 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 302.5 KiB, free 433.4 MiB)\n",
      "2023-12-17 15:16:12,601 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 53.4 KiB, free 433.3 MiB)\n",
      "2023-12-17 15:16:12,603 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 15d03baddaa3:42455 (size: 53.4 KiB, free: 434.2 MiB)\n",
      "2023-12-17 15:16:12,604 INFO spark.SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 15:16:12,609 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 15:16:12,624 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 15:16:12,625 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 15:16:12,625 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 15:16:12,625 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 15:16:12,625 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 15:16:12,627 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 15:16:12,649 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 23.5 KiB, free 433.3 MiB)\n",
      "2023-12-17 15:16:12,651 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 433.3 MiB)\n",
      "2023-12-17 15:16:12,652 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 15d03baddaa3:42455 (size: 9.4 KiB, free: 434.2 MiB)\n",
      "2023-12-17 15:16:12,653 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 15:16:12,653 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 15:16:12,653 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2023-12-17 15:16:12,655 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.6, executor 1, partition 0, PROCESS_LOCAL, 4917 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 15:16:12,698 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.6:43179 (size: 9.4 KiB, free: 127.2 MiB)\n",
      "2023-12-17 15:16:12,863 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.6:43179 (size: 53.4 KiB, free: 127.1 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|          2.1|         1|                 N|         142|          43|           2|        8.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          0.2|         1|                 N|         238|         151|           2|        3.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.3|                 0.0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|         14.7|         1|                 N|         132|         165|           1|       42.0|  0.5|    0.5|      8.65|         0.0|                  0.3|       51.95|                 0.0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|         10.6|         1|                 N|         138|         132|           1|       29.0|  0.5|    0.5|      6.05|         0.0|                  0.3|       36.35|                 0.0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|         0.0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|          1.6|         1|                 N|         224|          68|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|          4.1|         1|                 N|          95|         157|           2|       16.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        17.3|                 0.0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|          5.7|         1|                 N|          90|          40|           2|       18.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|          9.1|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        28.8|                 0.0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|          2.7|         1|                 N|         263|         142|           1|       12.0|  3.0|    0.5|      3.15|         0.0|                  0.3|       18.95|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 15:16:13,000 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 346 ms on 172.18.0.6 (executor 1) (1/1)\n",
      "2023-12-17 15:16:13,000 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-12-17 15:16:13,001 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.373 s\n",
      "2023-12-17 15:16:13,002 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 15:16:13,002 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2023-12-17 15:16:13,002 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.378465 s\n",
      "2023-12-17 15:16:13,051 INFO codegen.CodeGenerator: Code generated in 30.53915 ms\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 15:21:12,424 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-16 15:21:12,424 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-16 15:21:12,425 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: int, trip_distance: float ... 16 more fields>\n",
      "2023-12-16 15:21:12,551 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-16 15:21:12,578 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-16 15:21:12,578 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-16 15:21:12,579 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-16 15:21:12,579 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-16 15:21:12,579 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-16 15:21:12,580 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-16 15:21:12,835 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 302.5 KiB, free 434.1 MiB)\n",
      "2023-12-16 15:21:12,846 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 53.3 KiB, free 434.1 MiB)\n",
      "2023-12-16 15:21:12,848 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 14cd20eab8e3:44109 (size: 53.3 KiB, free: 434.3 MiB)\n",
      "2023-12-16 15:21:12,849 INFO spark.SparkContext: Created broadcast 7 from parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-16 15:21:12,851 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 43391889 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-16 15:21:12,900 INFO scheduler.DAGScheduler: Registering RDD 20 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-12-16 15:21:12,905 INFO scheduler.DAGScheduler: Got map stage job 3 (parquet at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "2023-12-16 15:21:12,905 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-16 15:21:12,906 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-16 15:21:12,906 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-16 15:21:12,909 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-16 15:21:12,953 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.6 KiB, free 434.0 MiB)\n",
      "2023-12-16 15:21:12,955 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 434.0 MiB)\n",
      "2023-12-16 15:21:12,957 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 14cd20eab8e3:44109 (size: 8.2 KiB, free: 434.3 MiB)\n",
      "2023-12-16 15:21:12,958 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-16 15:21:12,960 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "2023-12-16 15:21:12,960 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 3 tasks resource profile 0\n",
      "2023-12-16 15:21:12,963 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.19.0.6, executor 0, partition 0, PROCESS_LOCAL, 4906 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:12,964 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (172.19.0.8, executor 1, partition 1, PROCESS_LOCAL, 4906 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:12,964 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (172.19.0.7, executor 2, partition 2, PROCESS_LOCAL, 4906 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:12,996 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.0.6:35371 (size: 8.2 KiB, free: 127.2 MiB)\n",
      "2023-12-16 15:21:13,089 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.0.7:37405 (size: 8.2 KiB, free: 127.2 MiB)\n",
      "2023-12-16 15:21:13,263 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.0.6:35371 (size: 53.3 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:13,331 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.0.7:37405 (size: 53.3 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:13,424 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.0.8:33379 (size: 8.2 KiB, free: 127.2 MiB)\n",
      "2023-12-16 15:21:15,382 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.0.8:33379 (size: 53.3 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:18,253 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 5289 ms on 172.19.0.7 (executor 2) (1/3)\n",
      "2023-12-16 15:21:18,636 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 5674 ms on 172.19.0.6 (executor 0) (2/3)\n",
      "2023-12-16 15:21:21,940 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 8977 ms on 172.19.0.8 (executor 1) (3/3)\n",
      "2023-12-16 15:21:21,940 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-12-16 15:21:21,941 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 9.027 s\n",
      "2023-12-16 15:21:21,942 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-12-16 15:21:21,943 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-12-16 15:21:21,943 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-12-16 15:21:21,944 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-12-16 15:21:22,006 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-16 15:21:22,009 INFO scheduler.DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 24 output partitions\n",
      "2023-12-16 15:21:22,009 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-16 15:21:22,009 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "2023-12-16 15:21:22,009 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-16 15:21:22,011 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (ShuffledRowRDD[21] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-16 15:21:22,047 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 309.8 KiB, free 433.7 MiB)\n",
      "2023-12-16 15:21:22,050 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 113.4 KiB, free 433.6 MiB)\n",
      "2023-12-16 15:21:22,052 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 14cd20eab8e3:44109 (size: 113.4 KiB, free: 434.2 MiB)\n",
      "2023-12-16 15:21:22,052 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-16 15:21:22,053 INFO scheduler.DAGScheduler: Submitting 24 missing tasks from ResultStage 5 (ShuffledRowRDD[21] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-12-16 15:21:22,053 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 24 tasks resource profile 0\n",
      "2023-12-16 15:21:22,060 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (172.19.0.8, executor 1, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:22,061 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (172.19.0.6, executor 0, partition 1, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:22,062 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 8) (172.19.0.7, executor 2, partition 2, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:22,135 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.0.7:37405 (size: 113.4 KiB, free: 127.0 MiB)\n",
      "2023-12-16 15:21:22,150 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.0.6:35371 (size: 113.4 KiB, free: 127.0 MiB)\n",
      "2023-12-16 15:21:22,163 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.0.8:33379 (size: 113.4 KiB, free: 127.0 MiB)\n",
      "2023-12-16 15:21:22,413 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.0.6:46210\n",
      "2023-12-16 15:21:22,436 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.0.8:56368\n",
      "2023-12-16 15:21:22,446 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.0.7:44356\n",
      "2023-12-16 15:21:28,302 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 9) (172.19.0.7, executor 2, partition 3, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:28,308 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 10) (172.19.0.6, executor 0, partition 4, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:28,318 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.0 (TID 11) (172.19.0.8, executor 1, partition 5, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:28,320 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 8) in 6259 ms on 172.19.0.7 (executor 2) (1/24)\n",
      "2023-12-16 15:21:28,321 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 6260 ms on 172.19.0.6 (executor 0) (2/24)\n",
      "2023-12-16 15:21:28,325 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 6267 ms on 172.19.0.8 (executor 1) (3/24)\n",
      "2023-12-16 15:21:29,349 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 5.0 (TID 12) (172.19.0.7, executor 2, partition 6, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:29,350 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 9) in 1050 ms on 172.19.0.7 (executor 2) (4/24)\n",
      "2023-12-16 15:21:29,424 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 5.0 (TID 13) (172.19.0.6, executor 0, partition 7, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:29,426 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 10) in 1119 ms on 172.19.0.6 (executor 0) (5/24)\n",
      "2023-12-16 15:21:29,435 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 5.0 (TID 14) (172.19.0.8, executor 1, partition 8, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:29,436 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.0 (TID 11) in 1119 ms on 172.19.0.8 (executor 1) (6/24)\n",
      "2023-12-16 15:21:30,205 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 5.0 (TID 15) (172.19.0.7, executor 2, partition 9, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:30,207 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 5.0 (TID 12) in 859 ms on 172.19.0.7 (executor 2) (7/24)\n",
      "2023-12-16 15:21:30,295 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 5.0 (TID 16) (172.19.0.6, executor 0, partition 10, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:30,296 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 5.0 (TID 13) in 873 ms on 172.19.0.6 (executor 0) (8/24)\n",
      "2023-12-16 15:21:30,314 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 5.0 (TID 17) (172.19.0.8, executor 1, partition 11, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:30,315 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 5.0 (TID 14) in 881 ms on 172.19.0.8 (executor 1) (9/24)\n",
      "2023-12-16 15:21:30,985 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 5.0 (TID 18) (172.19.0.7, executor 2, partition 12, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:30,986 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 5.0 (TID 15) in 782 ms on 172.19.0.7 (executor 2) (10/24)\n",
      "2023-12-16 15:21:31,133 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 5.0 (TID 19) (172.19.0.8, executor 1, partition 13, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:31,134 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 5.0 (TID 17) in 821 ms on 172.19.0.8 (executor 1) (11/24)\n",
      "2023-12-16 15:21:31,142 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 5.0 (TID 20) (172.19.0.6, executor 0, partition 14, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:31,143 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 5.0 (TID 16) in 848 ms on 172.19.0.6 (executor 0) (12/24)\n",
      "2023-12-16 15:21:31,733 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 5.0 (TID 21) (172.19.0.7, executor 2, partition 15, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:31,735 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 5.0 (TID 18) in 751 ms on 172.19.0.7 (executor 2) (13/24)\n",
      "2023-12-16 15:21:31,883 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 5.0 (TID 22) (172.19.0.8, executor 1, partition 16, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:31,885 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 5.0 (TID 19) in 753 ms on 172.19.0.8 (executor 1) (14/24)\n",
      "2023-12-16 15:21:31,908 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 5.0 (TID 23) (172.19.0.6, executor 0, partition 17, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:31,909 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 5.0 (TID 20) in 768 ms on 172.19.0.6 (executor 0) (15/24)\n",
      "2023-12-16 15:21:32,528 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 5.0 (TID 24) (172.19.0.7, executor 2, partition 18, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:32,530 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 5.0 (TID 21) in 798 ms on 172.19.0.7 (executor 2) (16/24)\n",
      "2023-12-16 15:21:32,703 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 5.0 (TID 25) (172.19.0.8, executor 1, partition 19, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:32,704 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 5.0 (TID 22) in 822 ms on 172.19.0.8 (executor 1) (17/24)\n",
      "2023-12-16 15:21:32,757 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 5.0 (TID 26) (172.19.0.6, executor 0, partition 20, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:32,759 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 5.0 (TID 23) in 851 ms on 172.19.0.6 (executor 0) (18/24)\n",
      "2023-12-16 15:21:33,260 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 5.0 (TID 27) (172.19.0.7, executor 2, partition 21, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:33,262 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 5.0 (TID 24) in 735 ms on 172.19.0.7 (executor 2) (19/24)\n",
      "2023-12-16 15:21:33,442 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 5.0 (TID 28) (172.19.0.8, executor 1, partition 22, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:33,443 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 5.0 (TID 25) in 740 ms on 172.19.0.8 (executor 1) (20/24)\n",
      "2023-12-16 15:21:33,492 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 5.0 (TID 29) (172.19.0.6, executor 0, partition 23, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:33,493 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 5.0 (TID 26) in 736 ms on 172.19.0.6 (executor 0) (21/24)\n",
      "2023-12-16 15:21:34,046 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 5.0 (TID 27) in 786 ms on 172.19.0.7 (executor 2) (22/24)\n",
      "2023-12-16 15:21:34,304 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 5.0 (TID 28) in 863 ms on 172.19.0.8 (executor 1) (23/24)\n",
      "2023-12-16 15:21:34,333 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 5.0 (TID 29) in 842 ms on 172.19.0.6 (executor 0) (24/24)\n",
      "2023-12-16 15:21:34,333 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-12-16 15:21:34,334 INFO scheduler.DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 12.311 s\n",
      "2023-12-16 15:21:34,334 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-16 15:21:34,334 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "2023-12-16 15:21:34,336 INFO scheduler.DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 12.328306 s\n",
      "2023-12-16 15:21:34,343 INFO datasources.FileFormatWriter: Start to commit write Job c27dace5-8822-441b-b999-1c70cedd6080.\n",
      "2023-12-16 15:21:35,798 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 14cd20eab8e3:44109 in memory (size: 8.2 KiB, free: 434.2 MiB)\n",
      "2023-12-16 15:21:35,802 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 172.19.0.6:35371 in memory (size: 8.2 KiB, free: 127.0 MiB)\n",
      "2023-12-16 15:21:35,802 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 172.19.0.7:37405 in memory (size: 8.2 KiB, free: 127.0 MiB)\n",
      "2023-12-16 15:21:35,807 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 172.19.0.8:33379 in memory (size: 8.2 KiB, free: 127.0 MiB)\n",
      "2023-12-16 15:21:35,819 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 14cd20eab8e3:44109 in memory (size: 113.4 KiB, free: 434.3 MiB)\n",
      "2023-12-16 15:21:35,822 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 172.19.0.6:35371 in memory (size: 113.4 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:35,823 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 172.19.0.7:37405 in memory (size: 113.4 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:35,825 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 172.19.0.8:33379 in memory (size: 113.4 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:38,209 INFO datasources.FileFormatWriter: Write Job c27dace5-8822-441b-b999-1c70cedd6080 committed. Elapsed time: 3864 ms.\n",
      "2023-12-16 15:21:38,215 INFO datasources.FileFormatWriter: Finished processing stats for write job c27dace5-8822-441b-b999-1c70cedd6080.\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet('s3a://datalake/yellow_tripdata/2021/01/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 15:21:58,242 INFO datasources.InMemoryFileIndex: It took 15 ms to list leaf files for 1 paths.\n",
      "2023-12-16 15:21:58,319 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-16 15:21:58,320 INFO scheduler.DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-16 15:21:58,321 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-16 15:21:58,321 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-16 15:21:58,321 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-16 15:21:58,322 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-16 15:21:58,333 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 152.5 KiB, free 433.9 MiB)\n",
      "2023-12-16 15:21:58,336 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 55.8 KiB, free 433.8 MiB)\n",
      "2023-12-16 15:21:58,338 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 14cd20eab8e3:44109 (size: 55.8 KiB, free: 434.3 MiB)\n",
      "2023-12-16 15:21:58,339 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-16 15:21:58,340 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-16 15:21:58,340 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "2023-12-16 15:21:58,344 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 30) (172.19.0.6, executor 0, partition 0, PROCESS_LOCAL, 4658 bytes) taskResourceAssignments Map()\n",
      "2023-12-16 15:21:58,367 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.19.0.6:35371 (size: 55.8 KiB, free: 127.1 MiB)\n",
      "2023-12-16 15:21:58,521 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 30) in 180 ms on 172.19.0.6 (executor 0) (1/1)\n",
      "2023-12-16 15:21:58,522 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-12-16 15:21:58,523 INFO scheduler.DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.198 s\n",
      "2023-12-16 15:21:58,524 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-16 15:21:58,524 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "2023-12-16 15:21:58,524 INFO scheduler.DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.205091 s\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet('s3a://datalake/yellow_tripdata/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- improvement_surcharge: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- congestion_surcharge: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 03:28:29,043 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2023-12-18 03:28:29,186 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-18 03:28:29,273 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-18 03:28:29,274 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-12-18 03:28:29,274 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-18 03:28:29,275 INFO spark.SparkContext: Submitted application: Application\n",
      "2023-12-18 03:28:29,304 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-12-18 03:28:29,318 INFO resource.ResourceProfile: Limiting resource is cpu\n",
      "2023-12-18 03:28:29,319 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-12-18 03:28:29,381 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-12-18 03:28:29,382 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-12-18 03:28:29,383 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-12-18 03:28:29,384 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-12-18 03:28:29,384 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2023-12-18 03:28:29,648 INFO util.Utils: Successfully started service 'sparkDriver' on port 33879.\n",
      "2023-12-18 03:28:29,672 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-12-18 03:28:29,709 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-12-18 03:28:29,729 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-12-18 03:28:29,729 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-12-18 03:28:29,735 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-12-18 03:28:29,756 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-8fd44c6a-7c13-46c8-b871-7000a5ccaf8b\n",
      "2023-12-18 03:28:29,775 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-12-18 03:28:29,794 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-12-18 03:28:29,844 INFO util.log: Logging initialized @2529ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-12-18 03:28:29,977 INFO server.Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.21+9-post-Debian-1deb11u1\n",
      "2023-12-18 03:28:30,001 INFO server.Server: Started @2688ms\n",
      "2023-12-18 03:28:30,050 INFO server.AbstractConnector: Started ServerConnector@48ada9f2{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-12-18 03:28:30,050 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-12-18 03:28:30,081 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c3d7842{/,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,307 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "2023-12-18 03:28:30,388 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.19.0.4:7077 after 39 ms (0 ms spent in bootstraps)\n",
      "2023-12-18 03:28:30,526 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231218032830-0000\n",
      "2023-12-18 03:28:30,537 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46299.\n",
      "2023-12-18 03:28:30,537 INFO netty.NettyBlockTransferService: Server created on 230a73f7d3ae:46299\n",
      "2023-12-18 03:28:30,539 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-12-18 03:28:30,548 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231218032830-0000/0 on worker-20231218032725-172.19.0.7-7000 (172.19.0.7:7000) with 1 core(s)\n",
      "2023-12-18 03:28:30,550 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 230a73f7d3ae, 46299, None)\n",
      "2023-12-18 03:28:30,551 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231218032830-0000/0 on hostPort 172.19.0.7:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-18 03:28:30,552 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231218032830-0000/1 on worker-20231218032724-172.19.0.5-7000 (172.19.0.5:7000) with 1 core(s)\n",
      "2023-12-18 03:28:30,553 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231218032830-0000/1 on hostPort 172.19.0.5:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-18 03:28:30,556 INFO storage.BlockManagerMasterEndpoint: Registering block manager 230a73f7d3ae:46299 with 434.4 MiB RAM, BlockManagerId(driver, 230a73f7d3ae, 46299, None)\n",
      "2023-12-18 03:28:30,562 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 230a73f7d3ae, 46299, None)\n",
      "2023-12-18 03:28:30,565 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 230a73f7d3ae, 46299, None)\n",
      "2023-12-18 03:28:30,792 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231218032830-0000/0 is now RUNNING\n",
      "2023-12-18 03:28:30,792 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231218032830-0000/1 is now RUNNING\n",
      "2023-12-18 03:28:30,897 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1c3d7842{/,null,STOPPED,@Spark}\n",
      "2023-12-18 03:28:30,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c39476b{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,900 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@600898e4{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36cfa2e9{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,905 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@507ccc99{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,907 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f403c21{/stages,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,909 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3886f14e{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e80156d{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,913 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d5d0c83{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,914 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40effd05{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,916 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@326eb363{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,918 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66611cab{/storage,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,920 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2622ce3{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,922 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bec48f5{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,924 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2eb81789{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,925 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16f8d1c2{/environment,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,927 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34ff5b9b{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,929 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f911fb4{/executors,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,931 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3439f07{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,934 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66d76629{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,937 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45d400c1{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,953 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2caf9d7a{/static,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,955 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34bb8b28{/,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,958 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@330791b{/api,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,960 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21a8e33a{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,961 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45a02df{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,971 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61ae8c59{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-12-18 03:28:30,972 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "2023-12-18 03:28:34,028 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.7:41898) with ID 0,  ResourceProfileId 0\n",
      "2023-12-18 03:28:34,032 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.5:60200) with ID 1,  ResourceProfileId 0\n",
      "2023-12-18 03:28:34,277 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.19.0.7:39163 with 127.2 MiB RAM, BlockManagerId(0, 172.19.0.7, 39163, None)\n",
      "2023-12-18 03:28:34,280 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.19.0.5:38879 with 127.2 MiB RAM, BlockManagerId(1, 172.19.0.5, 38879, None)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"Application\"). \\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
