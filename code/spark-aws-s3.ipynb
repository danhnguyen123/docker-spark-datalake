{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39554c2-8e44-471b-9e57-e3557b709dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:53:34,635 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2023-12-17 09:53:34,769 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-17 09:53:34,864 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-17 09:53:34,864 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-12-17 09:53:34,864 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-17 09:53:34,865 INFO spark.SparkContext: Submitted application: Application\n",
      "2023-12-17 09:53:34,893 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-12-17 09:53:34,907 INFO resource.ResourceProfile: Limiting resource is cpu\n",
      "2023-12-17 09:53:34,907 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-12-17 09:53:34,970 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-12-17 09:53:34,970 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-12-17 09:53:34,971 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-12-17 09:53:34,972 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-12-17 09:53:34,972 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2023-12-17 09:53:35,261 INFO util.Utils: Successfully started service 'sparkDriver' on port 45251.\n",
      "2023-12-17 09:53:35,303 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-12-17 09:53:35,339 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-12-17 09:53:35,355 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-12-17 09:53:35,356 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-12-17 09:53:35,361 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-12-17 09:53:35,379 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f89f6c31-93c2-4408-ae57-ca687dd3b938\n",
      "2023-12-17 09:53:35,396 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-12-17 09:53:35,419 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-12-17 09:53:35,467 INFO util.log: Logging initialized @2620ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-12-17 09:53:35,597 INFO server.Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.21+9-post-Debian-1deb11u1\n",
      "2023-12-17 09:53:35,618 INFO server.Server: Started @2774ms\n",
      "2023-12-17 09:53:35,643 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-12-17 09:53:35,659 INFO server.AbstractConnector: Started ServerConnector@1684c589{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "2023-12-17 09:53:35,660 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "2023-12-17 09:53:35,689 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b7abccd{/,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:35,893 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "2023-12-17 09:53:35,980 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.22.0.3:7077 after 40 ms (0 ms spent in bootstraps)\n",
      "2023-12-17 09:53:36,087 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231217095336-0006\n",
      "2023-12-17 09:53:36,096 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36769.\n",
      "2023-12-17 09:53:36,096 INFO netty.NettyBlockTransferService: Server created on a31eab60218a:36769\n",
      "2023-12-17 09:53:36,098 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-12-17 09:53:36,104 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a31eab60218a, 36769, None)\n",
      "2023-12-17 09:53:36,109 INFO storage.BlockManagerMasterEndpoint: Registering block manager a31eab60218a:36769 with 434.4 MiB RAM, BlockManagerId(driver, a31eab60218a, 36769, None)\n",
      "2023-12-17 09:53:36,113 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a31eab60218a, 36769, None)\n",
      "2023-12-17 09:53:36,114 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, a31eab60218a, 36769, None)\n",
      "2023-12-17 09:53:36,986 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-12-17 09:53:37,015 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-12-17 09:53:37,016 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-12-17 09:53:40,626 INFO history.SingleEventLogFileWriter: Logging events to s3a://playground-datalake/spark-events/app-20231217095336-0006.inprogress\n",
      "2023-12-17 09:53:40,824 WARN s3a.S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark-events/app-20231217095336-0006.inprogress. This is Unsupported\n",
      "2023-12-17 09:53:40,895 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6b7abccd{/,null,STOPPED,@Spark}\n",
      "2023-12-17 09:53:40,897 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4af9c8b2{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,898 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66b7638d{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e95d33{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,901 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c974a89{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,902 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fca21af{/stages,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2701f328{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,905 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cd47fa9{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,907 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75b7c698{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,910 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b8429dc{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,915 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a14a8ed{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,916 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@359dc2f4{/storage,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,918 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ac726e0{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,919 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a8feb{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,925 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b51c757{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,926 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f4b9236{/environment,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,928 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@722b2b32{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,929 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b698099{/executors,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,930 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e140937{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,932 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1259e000{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,933 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77ec7405{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,951 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@410b4b61{/static,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,952 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@635e3e71{/,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,954 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55dffe2{/api,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,955 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54e70630{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,958 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ee56f8a{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,965 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57364265{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:53:40,967 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"Application\"). \\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.access.key\", f\"{os.environ['AWS_ACCESS_KEY_ID']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.secret.key\", f\"{os.environ['AWS_SECRET_ACCESS_KEY']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", f\"s3a://{os.environ['AWS_S3_BUCKET']}/spark-events/\").\\\n",
    "        config(\"spark.history.fs.logDirectory\", f\"s3a://{os.environ['AWS_S3_BUCKET']}/spark-events/\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c245fc6-9eb9-48c3-80ef-4b340b3ea7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.eventLog.enabled : true\n",
      "spark.executor.memory : 512m\n",
      "spark.driver.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.app.name : Application\n",
      "spark.eventLog.dir : s3a://spark-events/\n",
      "spark.app.submitTime : 1702803690431\n",
      "spark.hadoop.fs.s3a.endpoint : http://172.22.0.2:9000\n",
      "spark.executor.id : driver\n",
      "spark.app.id : app-20231217090134-0000\n",
      "spark.driver.host : a31eab60218a\n",
      "spark.driver.port : 33087\n",
      "spark.rdd.compress : True\n",
      "spark.master : spark://spark-master:7077\n",
      "spark.app.startTime : 1702803691015\n",
      "spark.hadoop.fs.s3a.access.key : minioadmin\n",
      "spark.executor.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.serializer.objectStreamReset : 100\n",
      "spark.submit.pyFiles : \n",
      "spark.submit.deployMode : client\n",
      "spark.history.fs.logDirectory : s3a://spark-events/\n",
      "spark.ui.showConsoleProgress : true\n",
      "spark.hadoop.fs.s3a.secret.key : minioadmin\n"
     ]
    }
   ],
   "source": [
    "conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Print Configuration\n",
    "for k,v in conf:\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615b5053-3784-4d16-bbf0-ad086e34d8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:54:43,424 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-12-17 09:54:43,427 INFO internal.SharedState: Warehouse path is 'file:/opt/workspace/code/spark-warehouse'.\n",
      "2023-12-17 09:54:43,456 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29fb52a3{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:54:43,457 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4720d1e2{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:54:43,459 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25ad430e{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:54:43,460 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@612463b0{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:54:43,463 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4dd1a1{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:54:47,050 INFO datasources.InMemoryFileIndex: It took 814 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:54:48,287 INFO datasources.InMemoryFileIndex: It took 759 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:54:51,379 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:54:51,382 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-12-17 09:54:51,385 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-17 09:54:52,173 INFO codegen.CodeGenerator: Code generated in 251.143135 ms\n",
      "2023-12-17 09:54:52,244 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 302.7 KiB, free 434.1 MiB)\n",
      "2023-12-17 09:54:52,336 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 53.6 KiB, free 434.1 MiB)\n",
      "2023-12-17 09:54:52,340 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on a31eab60218a:36769 (size: 53.6 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:54:52,345 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:54:52,360 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:54:52,558 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:54:52,593 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:54:52,594 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:54:52,595 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:54:52,597 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:54:52,607 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:54:52,732 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:54:52,768 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:54:52,769 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on a31eab60218a:36769 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:54:52,770 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:54:52,822 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:54:52,825 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:55:07,874 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2023-12-17 09:55:22,874 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2023-12-17 09:55:25,554 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231217095336-0006/0 on worker-20231217085357-172.22.0.5-7000 (172.22.0.5:7000) with 1 core(s)\n",
      "2023-12-17 09:55:25,557 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231217095336-0006/0 on hostPort 172.22.0.5:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-17 09:55:25,574 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231217095336-0006/1 on worker-20231217085358-172.22.0.6-7000 (172.22.0.6:7000) with 1 core(s)\n",
      "2023-12-17 09:55:25,575 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231217095336-0006/1 on hostPort 172.22.0.6:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-17 09:55:25,669 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231217095336-0006/0 is now RUNNING\n",
      "2023-12-17 09:55:25,682 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231217095336-0006/1 is now RUNNING\n",
      "2023-12-17 09:55:29,730 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.6:41064) with ID 1,  ResourceProfileId 0\n",
      "2023-12-17 09:55:29,738 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.5:60760) with ID 0,  ResourceProfileId 0\n",
      "2023-12-17 09:55:30,015 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.22.0.6:46351 with 127.2 MiB RAM, BlockManagerId(1, 172.22.0.6, 46351, None)\n",
      "2023-12-17 09:55:30,019 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.22.0.5:42271 with 127.2 MiB RAM, BlockManagerId(0, 172.22.0.5, 42271, None)\n",
      "2023-12-17 09:55:30,131 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:55:30,600 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.22.0.6:46351 (size: 5.9 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:55:31,542 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.6:46351 (size: 53.6 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:55:35,598 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5491 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:55:35,601 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:55:35,611 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 42.975 s\n",
      "2023-12-17 09:55:35,618 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:55:35,619 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-12-17 09:55:35,621 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 43.062507 s\n",
      "2023-12-17 09:55:35,676 INFO codegen.CodeGenerator: Code generated in 26.332181 ms\n",
      "2023-12-17 09:55:35,780 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:55:35,780 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:55:35,780 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-17 09:55:35,790 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 302.7 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:55:35,809 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 53.6 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:55:35,811 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on a31eab60218a:36769 (size: 53.6 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:55:35,813 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:55:35,815 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:55:35,927 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on a31eab60218a:36769 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:55:35,935 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.22.0.6:46351 in memory (size: 5.9 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:55:35,956 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on a31eab60218a:36769 in memory (size: 53.6 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:55:35,962 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 172.22.0.6:46351 in memory (size: 53.6 KiB, free: 127.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(f\"s3a://{os.environ['AWS_S3_BUCKET']}/yellow_tripdata_2021-01.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f353139-89d1-45c8-befb-1494babb559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:55:39,284 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:55:39,284 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:55:39,284 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:55:39,311 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 302.6 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:55:39,324 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 53.5 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:55:39,325 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on a31eab60218a:36769 (size: 53.5 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:55:39,326 INFO spark.SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:55:39,332 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:55:39,344 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:55:39,345 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:55:39,345 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:55:39,345 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:55:39,345 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:55:39,347 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:55:39,360 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.2 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:55:39,364 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:55:39,365 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on a31eab60218a:36769 (size: 6.2 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:55:39,366 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:55:39,367 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:55:39,367 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:55:39,369 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:55:39,618 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.22.0.5:42271 (size: 6.2 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:55:40,461 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.22.0.5:42271 (size: 53.5 KiB, free: 127.1 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|        10.60|         1|                 N|         138|         132|           1|         29|  0.5|    0.5|      6.05|           0|                  0.3|       36.35|                   0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|         1.60|         1|                 N|         224|          68|           1|          8|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|         4.10|         1|                 N|          95|         157|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        17.3|                   0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|         5.70|         1|                 N|          90|          40|           2|         18|    3|    0.5|         0|           0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|         9.10|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|         0|           0|                  0.3|        28.8|                   0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|         2.70|         1|                 N|         263|         142|           1|         12|    3|    0.5|      3.15|           0|                  0.3|       18.95|                 2.5|\n",
      "|       2| 2021-01-01 00:15:52|  2021-01-01 00:38:07|              3|         6.11|         1|                 N|         164|         255|           1|       20.5|  0.5|    0.5|         0|           0|                  0.3|        24.3|                 2.5|\n",
      "|       2| 2021-01-01 00:46:36|  2021-01-01 00:53:45|              2|         1.21|         1|                 N|         255|          80|           1|          7|  0.5|    0.5|      2.49|           0|                  0.3|       10.79|                   0|\n",
      "|       1| 2021-01-01 00:10:46|  2021-01-01 00:32:58|              2|         7.40|         1|                 N|         138|         166|           2|       24.5|  2.5|    0.5|         0|        6.12|                  0.3|       33.92|                   0|\n",
      "|       2| 2021-01-01 00:31:06|  2021-01-01 00:38:52|              5|         1.70|         1|                 N|         142|          50|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2021-01-01 00:42:11|  2021-01-01 00:44:24|              5|          .81|         1|                 N|          50|         142|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2021-01-01 00:17:48|  2021-01-01 00:21:55|              1|         1.01|         1|                 N|         236|         237|           1|        5.5|  0.5|    0.5|         1|           0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-01 00:33:38|  2021-01-01 00:38:37|              1|          .73|         1|                 N|         142|         239|           1|        5.5|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                 2.5|\n",
      "|       2| 2021-01-01 00:47:56|  2021-01-01 00:52:53|              1|         1.17|         1|                 N|         238|         166|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|\n",
      "|       2| 2021-01-01 00:04:21|  2021-01-01 00:07:58|              1|          .78|         1|                 N|         239|         238|           1|        4.5|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|                 2.5|\n",
      "|       2| 2021-01-01 00:18:36|  2021-01-01 00:27:10|              2|         1.66|         1|                 N|         151|         142|           2|        8.5|  0.5|    0.5|         0|           0|                  0.3|        12.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:55:45,386 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6018 ms on 172.22.0.5 (executor 0) (1/1)\n",
      "2023-12-17 09:55:45,386 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:55:45,388 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 6.038 s\n",
      "2023-12-17 09:55:45,388 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:55:45,388 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-12-17 09:55:45,389 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 6.045148 s\n",
      "2023-12-17 09:55:45,478 INFO codegen.CodeGenerator: Code generated in 54.668617 ms\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b6cb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:55:59,518 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:55:59,518 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:55:59,519 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:56:00,953 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:56:00,974 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-17 09:56:00,975 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-17 09:56:00,976 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:56:00,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-17 09:56:00,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-17 09:56:00,977 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:56:04,397 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 302.6 KiB, free 433.4 MiB)\n",
      "2023-12-17 09:56:04,408 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 53.5 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:56:04,409 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on a31eab60218a:36769 (size: 53.5 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:56:04,411 INFO spark.SparkContext: Created broadcast 5 from parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:56:04,412 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:56:04,446 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:56:04,447 INFO scheduler.DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-12-17 09:56:04,447 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:56:04,447 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:56:04,448 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:56:04,449 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:56:04,471 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 314.3 KiB, free 433.0 MiB)\n",
      "2023-12-17 09:56:04,476 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 115.3 KiB, free 432.9 MiB)\n",
      "2023-12-17 09:56:04,478 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on a31eab60218a:36769 (size: 115.3 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:56:04,479 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:56:04,480 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-12-17 09:56:04,480 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "2023-12-17 09:56:04,482 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:56:04,482 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.22.0.6, executor 1, partition 1, PROCESS_LOCAL, 4928 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:56:04,530 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.6:46351 (size: 115.3 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:56:04,531 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.5:42271 (size: 115.3 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:56:04,758 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.6:46351 (size: 53.5 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:56:06,726 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.5:42271 (size: 53.5 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:56:48,170 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 43688 ms on 172.22.0.5 (executor 0) (1/2)\n",
      "2023-12-17 09:57:04,164 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 59682 ms on 172.22.0.6 (executor 1) (2/2)\n",
      "2023-12-17 09:57:04,164 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:57:04,166 INFO scheduler.DAGScheduler: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 59.714 s\n",
      "2023-12-17 09:57:04,166 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:57:04,166 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2023-12-17 09:57:04,167 INFO scheduler.DAGScheduler: Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 59.720896 s\n",
      "2023-12-17 09:57:04,169 INFO datasources.FileFormatWriter: Start to commit write Job d3b4ccfa-af53-4858-aabf-d215ea6e028e.\n",
      "2023-12-17 09:57:26,339 INFO datasources.FileFormatWriter: Write Job d3b4ccfa-af53-4858-aabf-d215ea6e028e committed. Elapsed time: 22169 ms.\n",
      "2023-12-17 09:57:26,344 INFO datasources.FileFormatWriter: Finished processing stats for write job d3b4ccfa-af53-4858-aabf-d215ea6e028e.\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet(f\"s3a://{os.environ['AWS_S3_BUCKET']}/yellow_tripdata/2021/01/\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c5f5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:57:48,640 INFO datasources.InMemoryFileIndex: It took 380 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:57:48,702 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:57:48,715 INFO scheduler.DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:57:48,716 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:57:48,716 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:57:48,716 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:57:48,718 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:57:48,720 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on a31eab60218a:36769 in memory (size: 6.2 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:57:48,729 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 152.7 KiB, free 432.8 MiB)\n",
      "2023-12-17 09:57:48,730 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 172.22.0.5:42271 in memory (size: 6.2 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:57:48,732 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 55.8 KiB, free 432.7 MiB)\n",
      "2023-12-17 09:57:48,733 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on a31eab60218a:36769 (size: 55.8 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:57:48,734 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:57:48,735 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:57:48,735 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:57:48,738 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4669 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:57:48,741 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on a31eab60218a:36769 in memory (size: 53.6 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:57:48,752 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on a31eab60218a:36769 in memory (size: 53.5 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:57:48,757 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 172.22.0.5:42271 in memory (size: 53.5 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:57:48,767 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on a31eab60218a:36769 in memory (size: 53.5 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:57:48,768 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.22.0.6:46351 (size: 55.8 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:57:48,771 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.5:42271 in memory (size: 53.5 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:57:48,772 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.6:46351 in memory (size: 53.5 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:57:48,784 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on a31eab60218a:36769 in memory (size: 115.3 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:57:48,789 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.5:42271 in memory (size: 115.3 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:57:48,792 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.6:46351 in memory (size: 115.3 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:57:50,587 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 1851 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:57:50,588 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:57:50,588 INFO scheduler.DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.868 s\n",
      "2023-12-17 09:57:50,589 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:57:50,589 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "2023-12-17 09:57:50,589 INFO scheduler.DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.887419 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(f\"s3a://{os.environ['AWS_S3_BUCKET']}/yellow_tripdata/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b322f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:06:35,820 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:06:35,821 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:06:35,821 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:06:35,945 INFO codegen.CodeGenerator: Code generated in 65.567944 ms\n",
      "2023-12-17 09:06:35,957 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 306.1 KiB, free 433.1 MiB)\n",
      "2023-12-17 09:06:35,972 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 54.3 KiB, free 433.1 MiB)\n",
      "2023-12-17 09:06:35,974 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on a31eab60218a:43755 (size: 54.3 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:06:35,975 INFO spark.SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:06:35,981 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18086330 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:06:36,002 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:06:36,004 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:06:36,004 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:06:36,004 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:06:36,004 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:06:36,005 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:06:36,031 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 22.8 KiB, free 433.1 MiB)\n",
      "2023-12-17 09:06:36,034 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 433.0 MiB)\n",
      "2023-12-17 09:06:36,035 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on a31eab60218a:43755 (size: 7.1 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:06:36,036 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:06:36,037 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:06:36,037 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:06:36,039 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:06:36,098 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.22.0.6:45137 (size: 7.1 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:06:36,402 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.22.0.6:45137 (size: 54.3 KiB, free: 127.0 MiB)\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|        10.60|         1|                 N|         138|         132|           1|         29|  0.5|    0.5|      6.05|           0|                  0.3|       36.35|                   0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|         1.60|         1|                 N|         224|          68|           1|          8|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|         4.10|         1|                 N|          95|         157|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        17.3|                   0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|         5.70|         1|                 N|          90|          40|           2|         18|    3|    0.5|         0|           0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|         9.10|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|         0|           0|                  0.3|        28.8|                   0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|         2.70|         1|                 N|         263|         142|           1|         12|    3|    0.5|      3.15|           0|                  0.3|       18.95|                 2.5|\n",
      "|       2| 2021-01-01 00:15:52|  2021-01-01 00:38:07|              3|         6.11|         1|                 N|         164|         255|           1|       20.5|  0.5|    0.5|         0|           0|                  0.3|        24.3|                 2.5|\n",
      "|       2| 2021-01-01 00:46:36|  2021-01-01 00:53:45|              2|         1.21|         1|                 N|         255|          80|           1|          7|  0.5|    0.5|      2.49|           0|                  0.3|       10.79|                   0|\n",
      "|       1| 2021-01-01 00:10:46|  2021-01-01 00:32:58|              2|         7.40|         1|                 N|         138|         166|           2|       24.5|  2.5|    0.5|         0|        6.12|                  0.3|       33.92|                   0|\n",
      "|       2| 2021-01-01 00:31:06|  2021-01-01 00:38:52|              5|         1.70|         1|                 N|         142|          50|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2021-01-01 00:42:11|  2021-01-01 00:44:24|              5|          .81|         1|                 N|          50|         142|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2021-01-01 00:17:48|  2021-01-01 00:21:55|              1|         1.01|         1|                 N|         236|         237|           1|        5.5|  0.5|    0.5|         1|           0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-01 00:33:38|  2021-01-01 00:38:37|              1|          .73|         1|                 N|         142|         239|           1|        5.5|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                 2.5|\n",
      "|       2| 2021-01-01 00:47:56|  2021-01-01 00:52:53|              1|         1.17|         1|                 N|         238|         166|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|\n",
      "|       2| 2021-01-01 00:04:21|  2021-01-01 00:07:58|              1|          .78|         1|                 N|         239|         238|           1|        4.5|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|                 2.5|\n",
      "|       2| 2021-01-01 00:18:36|  2021-01-01 00:27:10|              2|         1.66|         1|                 N|         151|         142|           2|        8.5|  0.5|    0.5|         0|           0|                  0.3|        12.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:06:36,835 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 796 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:06:36,835 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:06:36,837 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.829 s\n",
      "2023-12-17 09:06:36,837 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:06:36,837 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2023-12-17 09:06:36,838 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.834913 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c694b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:58:54,463 INFO server.AbstractConnector: Stopped Spark@1684c589{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "2023-12-17 09:58:54,469 INFO ui.SparkUI: Stopped Spark web UI at http://a31eab60218a:4041\n",
      "2023-12-17 09:58:54,475 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors\n",
      "2023-12-17 09:58:54,475 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "2023-12-17 09:59:00,761 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-12-17 09:59:00,781 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-12-17 09:59:00,782 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-12-17 09:59:00,786 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-12-17 09:59:00,789 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-12-17 09:59:00,805 INFO spark.SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
