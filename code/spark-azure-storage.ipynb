{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39554c2-8e44-471b-9e57-e3557b709dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:35:00,520 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2023-12-17 09:35:00,672 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-17 09:35:00,769 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-17 09:35:00,769 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-12-17 09:35:00,770 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-17 09:35:00,771 INFO spark.SparkContext: Submitted application: My Application\n",
      "2023-12-17 09:35:00,804 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-12-17 09:35:00,824 INFO resource.ResourceProfile: Limiting resource is cpu\n",
      "2023-12-17 09:35:00,825 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-12-17 09:35:00,898 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-12-17 09:35:00,898 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-12-17 09:35:00,899 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-12-17 09:35:00,900 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-12-17 09:35:00,900 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2023-12-17 09:35:01,190 INFO util.Utils: Successfully started service 'sparkDriver' on port 45209.\n",
      "2023-12-17 09:35:01,216 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-12-17 09:35:01,247 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-12-17 09:35:01,261 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-12-17 09:35:01,262 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-12-17 09:35:01,266 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-12-17 09:35:01,284 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b54cd9d6-9f6f-4a53-8af1-016b6c98840a\n",
      "2023-12-17 09:35:01,300 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-12-17 09:35:01,320 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-12-17 09:35:01,369 INFO util.log: Logging initialized @2862ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-12-17 09:35:01,517 INFO server.Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.21+9-post-Debian-1deb11u1\n",
      "2023-12-17 09:35:01,548 INFO server.Server: Started @3043ms\n",
      "2023-12-17 09:35:01,598 INFO server.AbstractConnector: Started ServerConnector@39f5daed{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-12-17 09:35:01,598 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-12-17 09:35:01,627 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45a8efd3{/,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:01,875 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "2023-12-17 09:35:01,970 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.22.0.3:7077 after 46 ms (0 ms spent in bootstraps)\n",
      "2023-12-17 09:35:02,081 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231217093502-0004\n",
      "2023-12-17 09:35:02,084 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231217093502-0004/0 on worker-20231217085357-172.22.0.5-7000 (172.22.0.5:7000) with 1 core(s)\n",
      "2023-12-17 09:35:02,088 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231217093502-0004/0 on hostPort 172.22.0.5:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-17 09:35:02,090 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231217093502-0004/1 on worker-20231217085358-172.22.0.6-7000 (172.22.0.6:7000) with 1 core(s)\n",
      "2023-12-17 09:35:02,091 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231217093502-0004/1 on hostPort 172.22.0.6:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-17 09:35:02,096 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43207.\n",
      "2023-12-17 09:35:02,097 INFO netty.NettyBlockTransferService: Server created on a31eab60218a:43207\n",
      "2023-12-17 09:35:02,099 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-12-17 09:35:02,108 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a31eab60218a, 43207, None)\n",
      "2023-12-17 09:35:02,113 INFO storage.BlockManagerMasterEndpoint: Registering block manager a31eab60218a:43207 with 434.4 MiB RAM, BlockManagerId(driver, a31eab60218a, 43207, None)\n",
      "2023-12-17 09:35:02,118 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a31eab60218a, 43207, None)\n",
      "2023-12-17 09:35:02,120 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, a31eab60218a, 43207, None)\n",
      "2023-12-17 09:35:02,170 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231217093502-0004/1 is now RUNNING\n",
      "2023-12-17 09:35:02,174 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231217093502-0004/0 is now RUNNING\n",
      "2023-12-17 09:35:03,589 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-12-17 09:35:03,618 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-12-17 09:35:03,619 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-12-17 09:35:06,390 INFO history.SingleEventLogFileWriter: Logging events to s3a://spark-events/app-20231217093502-0004.inprogress\n",
      "2023-12-17 09:35:06,483 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.5:45788) with ID 0,  ResourceProfileId 0\n",
      "2023-12-17 09:35:06,488 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.6:49174) with ID 1,  ResourceProfileId 0\n",
      "2023-12-17 09:35:07,008 WARN s3a.S3ABlockOutputStream: Application invoked the Syncable API against stream writing to app-20231217093502-0004.inprogress. This is Unsupported\n",
      "2023-12-17 09:35:07,113 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@45a8efd3{/,null,STOPPED,@Spark}\n",
      "2023-12-17 09:35:07,119 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c3e21ae{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,121 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.22.0.5:37803 with 127.2 MiB RAM, BlockManagerId(0, 172.22.0.5, 37803, None)\n",
      "2023-12-17 09:35:07,123 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5609db29{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,131 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@87f7ae3{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,134 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.22.0.6:42433 with 127.2 MiB RAM, BlockManagerId(1, 172.22.0.6, 42433, None)\n",
      "2023-12-17 09:35:07,136 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7dcd88a9{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,141 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23bfdfa1{/stages,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,145 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7955f94a{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,149 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e52ce04{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,152 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31eac095{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,155 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ea4f33{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,159 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@486a83e8{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,164 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26592269{/storage,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,169 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d63be09{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,171 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@df14872{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,175 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c696d3f{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,177 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a68d6a4{/environment,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,180 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@118349a5{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,184 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dd24bcf{/executors,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,188 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16453668{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,196 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@432af43e{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,200 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65aa5d6f{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,241 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57a8b38b{/static,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,244 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f4a5552{/,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,250 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5eff1f48{/api,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,252 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2010c654{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,255 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d438c68{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,280 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1044e308{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:35:07,284 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 172.21.0.2: docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' minio\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"My Application\"). \\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.access.key\", f\"{os.environ['MINIO_ROOT_USER']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.secret.key\", f\"{os.environ['MINIO_ROOT_PASSWORD']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{os.environ['MINIO_IP']}:9000\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"s3a://spark-events/\").\\\n",
    "        config(\"spark.history.fs.logDirectory\", \"s3a://spark-events/\").\\\n",
    "        config(\"spark.hadoop.fs.AbstractFileSystem.wasb.Impl\", \"org.apache.hadoop.fs.azure.Wasb\").\\\n",
    "        config(f\"spark.hadoop.fs.azure.account.key.{os.environ['AZURE_STORAGE_ACCOUNT']}.blob.core.windows.net\", f\"{os.environ['AZURE_ACCESS_KEY']}\").\\\n",
    "        config(\"spark.hadoop.fs.azure.block.blob.with.compaction.dir\", \"/hbase/WALs,/data/myblobfiles\").\\\n",
    "        config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\").\\\n",
    "        config(\"spark.hadoop.fs.azure.enable.append.support\", \"true\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c245fc6-9eb9-48c3-80ef-4b340b3ea7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.hadoop.fs.azure.block.blob.with.compaction.dir : /hbase/WALs,/data/myblobfiles\n",
      "spark.executor.memory : 512m\n",
      "spark.eventLog.enabled : true\n",
      "spark.app.startTime : 1702805700506\n",
      "spark.eventLog.dir : s3a://spark-events/\n",
      "spark.driver.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.app.submitTime : 1702805700302\n",
      "spark.app.id : app-20231217093502-0004\n",
      "spark.hadoop.fs.s3a.endpoint : http://172.22.0.2:9000\n",
      "spark.executor.id : driver\n",
      "spark.hadoop.fs.azure.account.key.datalakeplaygroud.blob.core.windows.net : njaN56yPihnIc4VySTrNe40M8ZQckYML+QroNGSoU9qhZ/tfqs0ecih8/MkQfFhfH1IgixC5iZYP+AStQ72W/A==\n",
      "spark.driver.port : 45209\n",
      "spark.driver.host : a31eab60218a\n",
      "spark.rdd.compress : True\n",
      "spark.executor.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.master : spark://spark-master:7077\n",
      "spark.hadoop.fs.AbstractFileSystem.wasb.Impl : org.apache.hadoop.fs.azure.Wasb\n",
      "spark.app.name : My Application\n",
      "spark.hadoop.fs.s3a.access.key : minioadmin\n",
      "spark.serializer.objectStreamReset : 100\n",
      "spark.submit.pyFiles : \n",
      "spark.submit.deployMode : client\n",
      "spark.hadoop.fs.azure.enable.append.support : true\n",
      "spark.history.fs.logDirectory : s3a://spark-events/\n",
      "spark.ui.showConsoleProgress : true\n",
      "spark.hadoop.fs.azure : org.apache.hadoop.fs.azure.NativeAzureFileSystem\n",
      "spark.hadoop.fs.s3a.secret.key : minioadmin\n"
     ]
    }
   ],
   "source": [
    "conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Print Configuration\n",
    "for k,v in conf:\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f935c777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:38:32,752 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-12-17 09:38:32,756 INFO internal.SharedState: Warehouse path is 'file:/opt/workspace/code/spark-warehouse'.\n",
      "2023-12-17 09:38:32,776 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@247b1c5b{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:38:32,777 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50ecd47b{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:38:32,778 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@292a0769{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:38:32,779 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a0ee75d{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:38:32,781 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2191bcec{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:38:33,688 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-12-17 09:38:33,691 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-12-17 09:38:33,691 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "2023-12-17 09:38:37,024 INFO datasources.InMemoryFileIndex: It took 596 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:38:38,210 INFO datasources.InMemoryFileIndex: It took 561 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:38:41,085 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:38:41,088 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-12-17 09:38:41,091 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-17 09:38:41,784 INFO codegen.CodeGenerator: Code generated in 234.699426 ms\n",
      "2023-12-17 09:38:41,882 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 304.6 KiB, free 434.1 MiB)\n",
      "2023-12-17 09:38:41,992 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 54.0 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:38:41,996 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on a31eab60218a:43207 (size: 54.0 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:38:42,002 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:38:42,017 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:38:42,148 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:38:42,167 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:38:42,168 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:38:42,168 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:38:42,169 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:38:42,178 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:38:42,286 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:38:42,329 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:38:42,331 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on a31eab60218a:43207 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:38:42,332 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:38:42,367 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:38:42,369 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:38:42,446 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:38:43,074 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.22.0.5:37803 (size: 5.9 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:38:44,339 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.5:37803 (size: 54.0 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:39:13,163 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 30740 ms on 172.22.0.5 (executor 0) (1/1)\n",
      "2023-12-17 09:39:13,166 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:39:13,172 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 30.970 s\n",
      "2023-12-17 09:39:13,180 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:39:13,181 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-12-17 09:39:13,184 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 31.036768 s\n",
      "2023-12-17 09:39:13,224 INFO codegen.CodeGenerator: Code generated in 15.286314 ms\n",
      "2023-12-17 09:39:13,293 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:39:13,294 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:39:13,294 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-17 09:39:13,304 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 304.6 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:39:13,346 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 54.0 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:39:13,348 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on a31eab60218a:43207 (size: 54.0 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:39:13,349 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:39:13,351 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(f\"wasbs://{os.environ['AZURE_STORAGE_CONTAINER']}@{os.environ['AZURE_STORAGE_ACCOUNT']}.blob.core.windows.net/yellow_tripdata_2021-01.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0ddc23-8183-4bb6-8977-7c84e7d40773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:39:23,366 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:39:23,367 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:39:23,367 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:39:23,389 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 304.5 KiB, free 433.4 MiB)\n",
      "2023-12-17 09:39:23,400 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 54.0 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:39:23,401 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on a31eab60218a:43207 (size: 54.0 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:39:23,402 INFO spark.SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:39:23,407 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:39:23,417 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:39:23,419 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:39:23,419 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:39:23,419 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:39:23,419 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:39:23,420 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:39:23,431 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.2 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:39:23,434 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:39:23,436 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on a31eab60218a:43207 (size: 6.2 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:39:23,436 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:39:23,437 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:39:23,438 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:39:23,439 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:39:23,480 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.22.0.5:37803 (size: 6.2 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:39:23,576 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.22.0.5:37803 (size: 54.0 KiB, free: 127.1 MiB)\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|        10.60|         1|                 N|         138|         132|           1|         29|  0.5|    0.5|      6.05|           0|                  0.3|       36.35|                   0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|         1.60|         1|                 N|         224|          68|           1|          8|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|         4.10|         1|                 N|          95|         157|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        17.3|                   0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|         5.70|         1|                 N|          90|          40|           2|         18|    3|    0.5|         0|           0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|         9.10|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|         0|           0|                  0.3|        28.8|                   0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|         2.70|         1|                 N|         263|         142|           1|         12|    3|    0.5|      3.15|           0|                  0.3|       18.95|                 2.5|\n",
      "|       2| 2021-01-01 00:15:52|  2021-01-01 00:38:07|              3|         6.11|         1|                 N|         164|         255|           1|       20.5|  0.5|    0.5|         0|           0|                  0.3|        24.3|                 2.5|\n",
      "|       2| 2021-01-01 00:46:36|  2021-01-01 00:53:45|              2|         1.21|         1|                 N|         255|          80|           1|          7|  0.5|    0.5|      2.49|           0|                  0.3|       10.79|                   0|\n",
      "|       1| 2021-01-01 00:10:46|  2021-01-01 00:32:58|              2|         7.40|         1|                 N|         138|         166|           2|       24.5|  2.5|    0.5|         0|        6.12|                  0.3|       33.92|                   0|\n",
      "|       2| 2021-01-01 00:31:06|  2021-01-01 00:38:52|              5|         1.70|         1|                 N|         142|          50|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2021-01-01 00:42:11|  2021-01-01 00:44:24|              5|          .81|         1|                 N|          50|         142|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2021-01-01 00:17:48|  2021-01-01 00:21:55|              1|         1.01|         1|                 N|         236|         237|           1|        5.5|  0.5|    0.5|         1|           0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-01 00:33:38|  2021-01-01 00:38:37|              1|          .73|         1|                 N|         142|         239|           1|        5.5|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                 2.5|\n",
      "|       2| 2021-01-01 00:47:56|  2021-01-01 00:52:53|              1|         1.17|         1|                 N|         238|         166|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|\n",
      "|       2| 2021-01-01 00:04:21|  2021-01-01 00:07:58|              1|          .78|         1|                 N|         239|         238|           1|        4.5|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|                 2.5|\n",
      "|       2| 2021-01-01 00:18:36|  2021-01-01 00:27:10|              2|         1.66|         1|                 N|         151|         142|           2|        8.5|  0.5|    0.5|         0|           0|                  0.3|        12.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:39:59,983 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 36544 ms on 172.22.0.5 (executor 0) (1/1)\n",
      "2023-12-17 09:39:59,983 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:39:59,985 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 36.563 s\n",
      "2023-12-17 09:39:59,985 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:39:59,985 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-12-17 09:39:59,986 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 36.568292 s\n",
      "2023-12-17 09:40:00,056 INFO codegen.CodeGenerator: Code generated in 54.085607 ms\n",
      "2023-12-17 09:40:00,078 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on a31eab60218a:43207 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:40:00,086 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.22.0.5:37803 in memory (size: 5.9 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:40:00,109 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on a31eab60218a:43207 in memory (size: 54.0 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:40:00,114 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 172.22.0.5:37803 in memory (size: 54.0 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:40:00,134 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on a31eab60218a:43207 in memory (size: 6.2 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:40:00,136 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 172.22.0.5:37803 in memory (size: 6.2 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:40:00,153 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on a31eab60218a:43207 in memory (size: 54.0 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb8f687-4c8f-4bcd-8a5c-baf2e00574db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:40:21,528 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:40:21,529 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:40:21,529 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:40:23,080 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:40:23,101 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-17 09:40:23,101 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-17 09:40:23,102 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:40:23,102 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-17 09:40:23,102 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-17 09:40:23,103 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:40:31,036 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 304.5 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:40:31,052 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 54.0 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:40:31,053 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on a31eab60218a:43207 (size: 54.0 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:40:31,055 INFO spark.SparkContext: Created broadcast 5 from parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:40:31,057 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:40:31,091 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:40:31,092 INFO scheduler.DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-12-17 09:40:31,092 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:40:31,092 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:40:31,093 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:40:31,095 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:40:31,115 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 316.6 KiB, free 433.4 MiB)\n",
      "2023-12-17 09:40:31,120 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 116.5 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:40:31,121 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on a31eab60218a:43207 (size: 116.5 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:40:31,122 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:40:31,123 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-12-17 09:40:31,123 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "2023-12-17 09:40:31,125 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:40:31,126 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.22.0.5, executor 0, partition 1, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:40:31,161 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.5:37803 (size: 116.5 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:40:31,346 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.5:37803 (size: 54.0 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:40:31,704 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.6:42433 (size: 116.5 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:40:46,903 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.6:42433 (size: 54.0 KiB, free: 127.0 MiB)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 2) / 2]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwasbs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAZURE_STORAGE_CONTAINER\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m@\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAZURE_STORAGE_ACCOUNT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.blob.core.windows.net/yellow_tripdata/2021/01/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(f\"wasbs://{os.environ['AZURE_STORAGE_CONTAINER']}@{os.environ['AZURE_STORAGE_ACCOUNT']}.blob.core.windows.net/yellow_tripdata/2021/01/\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c37d24-bd29-432e-a449-07d6910c1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:27:59,868 INFO datasources.InMemoryFileIndex: It took 554 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:27:59,935 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:59,936 INFO scheduler.DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:27:59,936 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:27:59,936 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:27:59,937 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:27:59,938 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:27:59,957 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 153.3 KiB, free 433.9 MiB)\n",
      "2023-12-17 09:27:59,967 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 56.1 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:27:59,969 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on a31eab60218a:42037 (size: 56.1 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:59,970 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on a31eab60218a:42037 in memory (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:59,970 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:27:59,971 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:27:59,972 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:27:59,973 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.5:46397 in memory (size: 53.8 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:27:59,976 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.6:45463 in memory (size: 53.8 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:27:59,976 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4668 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:28:00,042 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.22.0.6:45463 (size: 56.1 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:28:02,317 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 2344 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:28:02,317 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:28:02,318 INFO scheduler.DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.378 s\n",
      "2023-12-17 09:28:02,318 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:28:02,318 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "2023-12-17 09:28:02,319 INFO scheduler.DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.383461 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(f\"wasbs://{os.environ['AZURE_STORAGE_CONTAINER']}@{os.environ['AZURE_STORAGE_ACCOUNT']}.blob.core.windows.net/yellow_tripdata/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5847c287-629c-43d2-8b5e-3dfb2763da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:28:02,386 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:28:02,386 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:28:02,386 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:28:02,459 INFO codegen.CodeGenerator: Code generated in 40.494551 ms\n",
      "2023-12-17 09:28:02,469 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 307.8 KiB, free 433.9 MiB)\n",
      "2023-12-17 09:28:02,480 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 54.3 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:28:02,482 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on a31eab60218a:42037 (size: 54.3 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:28:02,483 INFO spark.SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:28:02,487 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18086330 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:28:02,501 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:28:02,503 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:28:02,503 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:28:02,503 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:28:02,504 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:28:02,505 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:28:02,527 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 22.8 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:28:02,530 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:28:02,531 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on a31eab60218a:42037 (size: 7.1 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:28:02,532 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:28:02,533 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:28:02,533 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:28:02,535 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4991 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:28:02,559 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.22.0.6:45463 (size: 7.1 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:28:02,691 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.22.0.6:45463 (size: 54.3 KiB, free: 127.1 MiB)\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|        10.60|         1|                 N|         138|         132|           1|         29|  0.5|    0.5|      6.05|           0|                  0.3|       36.35|                   0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|         1.60|         1|                 N|         224|          68|           1|          8|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|         4.10|         1|                 N|          95|         157|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        17.3|                   0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|         5.70|         1|                 N|          90|          40|           2|         18|    3|    0.5|         0|           0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|         9.10|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|         0|           0|                  0.3|        28.8|                   0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|         2.70|         1|                 N|         263|         142|           1|         12|    3|    0.5|      3.15|           0|                  0.3|       18.95|                 2.5|\n",
      "|       2| 2021-01-01 00:15:52|  2021-01-01 00:38:07|              3|         6.11|         1|                 N|         164|         255|           1|       20.5|  0.5|    0.5|         0|           0|                  0.3|        24.3|                 2.5|\n",
      "|       2| 2021-01-01 00:46:36|  2021-01-01 00:53:45|              2|         1.21|         1|                 N|         255|          80|           1|          7|  0.5|    0.5|      2.49|           0|                  0.3|       10.79|                   0|\n",
      "|       1| 2021-01-01 00:10:46|  2021-01-01 00:32:58|              2|         7.40|         1|                 N|         138|         166|           2|       24.5|  2.5|    0.5|         0|        6.12|                  0.3|       33.92|                   0|\n",
      "|       2| 2021-01-01 00:31:06|  2021-01-01 00:38:52|              5|         1.70|         1|                 N|         142|          50|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2021-01-01 00:42:11|  2021-01-01 00:44:24|              5|          .81|         1|                 N|          50|         142|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2021-01-01 00:17:48|  2021-01-01 00:21:55|              1|         1.01|         1|                 N|         236|         237|           1|        5.5|  0.5|    0.5|         1|           0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-01 00:33:38|  2021-01-01 00:38:37|              1|          .73|         1|                 N|         142|         239|           1|        5.5|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                 2.5|\n",
      "|       2| 2021-01-01 00:47:56|  2021-01-01 00:52:53|              1|         1.17|         1|                 N|         238|         166|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|\n",
      "|       2| 2021-01-01 00:04:21|  2021-01-01 00:07:58|              1|          .78|         1|                 N|         239|         238|           1|        4.5|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|                 2.5|\n",
      "|       2| 2021-01-01 00:18:36|  2021-01-01 00:27:10|              2|         1.66|         1|                 N|         151|         142|           2|        8.5|  0.5|    0.5|         0|           0|                  0.3|        12.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:28:10,555 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 8021 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:28:10,555 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:28:10,556 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 8.050 s\n",
      "2023-12-17 09:28:10,556 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:28:10,556 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2023-12-17 09:28:10,557 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 8.055501 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33bbce25-e2e8-46d9-8abb-dfc2fde658f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:28:10,604 INFO server.AbstractConnector: Stopped Spark@411e682a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-12-17 09:28:10,609 INFO ui.SparkUI: Stopped Spark web UI at http://a31eab60218a:4040\n",
      "2023-12-17 09:28:10,615 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors\n",
      "2023-12-17 09:28:10,616 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "2023-12-17 09:28:11,040 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-12-17 09:28:11,056 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-12-17 09:28:11,057 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-12-17 09:28:11,060 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-12-17 09:28:11,064 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-12-17 09:28:11,078 INFO spark.SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
