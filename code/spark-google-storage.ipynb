{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39554c2-8e44-471b-9e57-e3557b709dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:20:53,105 INFO spark.SparkContext: Running Spark version 3.3.0\n",
      "2023-12-17 09:20:53,248 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-12-17 09:20:53,351 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-17 09:20:53,352 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-12-17 09:20:53,352 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-12-17 09:20:53,353 INFO spark.SparkContext: Submitted application: My Application\n",
      "2023-12-17 09:20:53,388 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-12-17 09:20:53,406 INFO resource.ResourceProfile: Limiting resource is cpu\n",
      "2023-12-17 09:20:53,406 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-12-17 09:20:53,482 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-12-17 09:20:53,483 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-12-17 09:20:53,484 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-12-17 09:20:53,484 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-12-17 09:20:53,485 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2023-12-17 09:20:53,792 INFO util.Utils: Successfully started service 'sparkDriver' on port 38643.\n",
      "2023-12-17 09:20:53,819 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-12-17 09:20:53,850 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-12-17 09:20:53,863 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-12-17 09:20:53,863 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-12-17 09:20:53,868 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-12-17 09:20:53,886 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1e1fa27c-ce91-450e-b2b7-9634fb5b9927\n",
      "2023-12-17 09:20:53,901 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-12-17 09:20:53,918 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-12-17 09:20:53,963 INFO util.log: Logging initialized @2823ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-12-17 09:20:54,104 INFO server.Server: jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.21+9-post-Debian-1deb11u1\n",
      "2023-12-17 09:20:54,132 INFO server.Server: Started @2995ms\n",
      "2023-12-17 09:20:54,181 INFO server.AbstractConnector: Started ServerConnector@411e682a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-12-17 09:20:54,182 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-12-17 09:20:54,213 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ce4e707{/,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:54,475 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "2023-12-17 09:20:54,573 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.22.0.3:7077 after 45 ms (0 ms spent in bootstraps)\n",
      "2023-12-17 09:20:54,675 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20231217092054-0003\n",
      "2023-12-17 09:20:54,679 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231217092054-0003/0 on worker-20231217085357-172.22.0.5-7000 (172.22.0.5:7000) with 1 core(s)\n",
      "2023-12-17 09:20:54,683 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231217092054-0003/0 on hostPort 172.22.0.5:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-17 09:20:54,685 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20231217092054-0003/1 on worker-20231217085358-172.22.0.6-7000 (172.22.0.6:7000) with 1 core(s)\n",
      "2023-12-17 09:20:54,685 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20231217092054-0003/1 on hostPort 172.22.0.6:7000 with 1 core(s), 512.0 MiB RAM\n",
      "2023-12-17 09:20:54,691 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42037.\n",
      "2023-12-17 09:20:54,691 INFO netty.NettyBlockTransferService: Server created on a31eab60218a:42037\n",
      "2023-12-17 09:20:54,694 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-12-17 09:20:54,709 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a31eab60218a, 42037, None)\n",
      "2023-12-17 09:20:54,717 INFO storage.BlockManagerMasterEndpoint: Registering block manager a31eab60218a:42037 with 434.4 MiB RAM, BlockManagerId(driver, a31eab60218a, 42037, None)\n",
      "2023-12-17 09:20:54,724 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a31eab60218a, 42037, None)\n",
      "2023-12-17 09:20:54,726 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, a31eab60218a, 42037, None)\n",
      "2023-12-17 09:20:54,749 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231217092054-0003/0 is now RUNNING\n",
      "2023-12-17 09:20:54,754 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20231217092054-0003/1 is now RUNNING\n",
      "2023-12-17 09:20:56,035 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-12-17 09:20:56,072 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-12-17 09:20:56,073 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-12-17 09:20:58,556 INFO history.SingleEventLogFileWriter: Logging events to s3a://spark-events/app-20231217092054-0003.inprogress\n",
      "2023-12-17 09:20:58,779 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.6:36744) with ID 1,  ResourceProfileId 0\n",
      "2023-12-17 09:20:58,783 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.22.0.5:48962) with ID 0,  ResourceProfileId 0\n",
      "2023-12-17 09:20:58,873 WARN s3a.S3ABlockOutputStream: Application invoked the Syncable API against stream writing to app-20231217092054-0003.inprogress. This is Unsupported\n",
      "2023-12-17 09:20:58,933 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3ce4e707{/,null,STOPPED,@Spark}\n",
      "2023-12-17 09:20:58,935 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fb11898{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,939 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c10a656{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,942 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@405b034e{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,948 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c8234cb{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,951 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e3f07df{/stages,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,953 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e533eec{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,956 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21423339{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,959 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29f3c776{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,961 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25568870{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,963 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ca59672{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,965 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2534fb0f{/storage,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,967 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f82725d{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,969 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e685438{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,972 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ec27aeb{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ffdbd4e{/environment,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,979 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67ade163{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,982 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51c0272e{/executors,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,985 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dd0b1f4{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,989 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@317e2485{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:58,991 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bd31a88{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,016 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@567500d6{/static,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,018 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46279eb8{/,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,021 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@200f3b9e{/api,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,029 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48f796fd{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,032 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6987eafd{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,045 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28a58871{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:20:59,048 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "2023-12-17 09:20:59,088 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.22.0.6:45463 with 127.2 MiB RAM, BlockManagerId(1, 172.22.0.6, 45463, None)\n",
      "2023-12-17 09:20:59,117 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.22.0.5:46397 with 127.2 MiB RAM, BlockManagerId(0, 172.22.0.5, 46397, None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 172.21.0.2: docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' minio\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"My Application\"). \\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.access.key\", f\"{os.environ['MINIO_ROOT_USER']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.secret.key\", f\"{os.environ['MINIO_ROOT_PASSWORD']}\").\\\n",
    "        config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{os.environ['MINIO_IP']}:9000\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"s3a://spark-events/\").\\\n",
    "        config(\"spark.history.fs.logDirectory\", \"s3a://spark-events/\").\\\n",
    "        config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\").\\\n",
    "        config(\"spark.hadoop.fs.gs.project.id\", f\"{os.environ['GOOGLE_PROJECT_ID']}\").\\\n",
    "        config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\").\\\n",
    "        config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", f\"{os.environ['GOOGLE_SERVICE_ACCOUNT_KEY_PATH']}\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c245fc6-9eb9-48c3-80ef-4b340b3ea7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.eventLog.enabled : true\n",
      "spark.executor.memory : 512m\n",
      "spark.driver.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.eventLog.dir : s3a://spark-events/\n",
      "spark.app.startTime : 1702804853093\n",
      "spark.hadoop.fs.AbstractFileSystem.gs.impl : com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\n",
      "spark.app.submitTime : 1702804852892\n",
      "spark.hadoop.fs.s3a.endpoint : http://172.22.0.2:9000\n",
      "spark.driver.port : 38643\n",
      "spark.executor.id : driver\n",
      "spark.driver.host : a31eab60218a\n",
      "spark.hadoop.fs.gs.project.id : danh-nguyen-403304\n",
      "spark.app.id : app-20231217092054-0003\n",
      "spark.rdd.compress : True\n",
      "spark.executor.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.master : spark://spark-master:7077\n",
      "spark.app.name : My Application\n",
      "spark.hadoop.fs.s3a.access.key : minioadmin\n",
      "spark.serializer.objectStreamReset : 100\n",
      "spark.submit.pyFiles : \n",
      "spark.submit.deployMode : client\n",
      "spark.hadoop.google.cloud.auth.service.account.enable : true\n",
      "spark.history.fs.logDirectory : s3a://spark-events/\n",
      "spark.ui.showConsoleProgress : true\n",
      "spark.hadoop.google.cloud.auth.service.account.json.keyfile : /opt/workspace/code/service-account.json\n",
      "spark.hadoop.fs.s3a.secret.key : minioadmin\n"
     ]
    }
   ],
   "source": [
    "conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Print Configuration\n",
    "for k,v in conf:\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f935c777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:26:57,025 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-12-17 09:26:57,031 INFO internal.SharedState: Warehouse path is 'file:/opt/workspace/code/spark-warehouse'.\n",
      "2023-12-17 09:26:57,048 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fe3703c{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:26:57,049 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35a5eb86{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:26:57,050 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@469e0f50{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:26:57,051 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a14561{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:26:57,053 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c4b464c{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-12-17 09:26:59,022 INFO gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=915; previousMaxLatencyMs=0; operationCount=1; context=gs://datalake-playground/yellow_tripdata_2021-01.csv\n",
      "2023-12-17 09:26:59,919 INFO datasources.InMemoryFileIndex: It took 278 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:27:00,841 INFO datasources.InMemoryFileIndex: It took 338 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:27:03,163 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:27:03,165 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-12-17 09:27:03,168 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-17 09:27:03,700 INFO codegen.CodeGenerator: Code generated in 162.732632 ms\n",
      "2023-12-17 09:27:03,763 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 304.1 KiB, free 434.1 MiB)\n",
      "2023-12-17 09:27:03,825 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 434.1 MiB)\n",
      "2023-12-17 09:27:03,830 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on a31eab60218a:42037 (size: 53.7 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:03,836 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:03,851 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:27:03,985 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:04,005 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:27:04,005 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:27:04,006 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:27:04,007 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:27:04,013 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:27:04,087 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:27:04,092 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.0 MiB)\n",
      "2023-12-17 09:27:04,094 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on a31eab60218a:42037 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:04,095 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:27:04,114 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:27:04,115 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:27:04,160 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4927 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:27:04,432 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.22.0.5:46397 (size: 5.9 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:27:05,248 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.22.0.5:46397 (size: 53.7 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:08,072 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3925 ms on 172.22.0.5 (executor 0) (1/1)\n",
      "2023-12-17 09:27:08,076 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:27:08,084 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 4.055 s\n",
      "2023-12-17 09:27:08,088 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:27:08,089 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-12-17 09:27:08,092 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 4.105654 s\n",
      "2023-12-17 09:27:08,140 INFO codegen.CodeGenerator: Code generated in 29.811843 ms\n",
      "2023-12-17 09:27:08,170 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on a31eab60218a:42037 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:08,177 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.22.0.5:46397 in memory (size: 5.9 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:08,221 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:27:08,222 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:27:08,222 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-12-17 09:27:08,233 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 304.1 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:27:08,249 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 53.7 KiB, free 433.7 MiB)\n",
      "2023-12-17 09:27:08,251 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on a31eab60218a:42037 (size: 53.7 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:08,252 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:08,253 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(f\"gs://{os.environ['GOOGLE_STORAGE_BUCKET']}/yellow_tripdata_2021-01.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0ddc23-8183-4bb6-8977-7c84e7d40773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:27:10,623 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:27:10,623 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:27:10,624 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:27:10,646 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 304.0 KiB, free 433.4 MiB)\n",
      "2023-12-17 09:27:10,660 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 53.8 KiB, free 433.4 MiB)\n",
      "2023-12-17 09:27:10,662 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on a31eab60218a:42037 (size: 53.8 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:27:10,663 INFO spark.SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:10,668 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:27:10,677 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:10,678 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:27:10,679 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:27:10,679 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:27:10,679 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:27:10,681 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:27:10,692 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.2 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:27:10,695 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 433.3 MiB)\n",
      "2023-12-17 09:27:10,697 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on a31eab60218a:42037 (size: 6.2 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:27:10,697 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:27:10,698 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:27:10,699 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:27:10,701 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4927 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:27:10,744 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.22.0.5:46397 (size: 6.2 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:10,860 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.22.0.5:46397 (size: 53.8 KiB, free: 127.1 MiB)\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|        10.60|         1|                 N|         138|         132|           1|         29|  0.5|    0.5|      6.05|           0|                  0.3|       36.35|                   0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|         1.60|         1|                 N|         224|          68|           1|          8|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|         4.10|         1|                 N|          95|         157|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        17.3|                   0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|         5.70|         1|                 N|          90|          40|           2|         18|    3|    0.5|         0|           0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|         9.10|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|         0|           0|                  0.3|        28.8|                   0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|         2.70|         1|                 N|         263|         142|           1|         12|    3|    0.5|      3.15|           0|                  0.3|       18.95|                 2.5|\n",
      "|       2| 2021-01-01 00:15:52|  2021-01-01 00:38:07|              3|         6.11|         1|                 N|         164|         255|           1|       20.5|  0.5|    0.5|         0|           0|                  0.3|        24.3|                 2.5|\n",
      "|       2| 2021-01-01 00:46:36|  2021-01-01 00:53:45|              2|         1.21|         1|                 N|         255|          80|           1|          7|  0.5|    0.5|      2.49|           0|                  0.3|       10.79|                   0|\n",
      "|       1| 2021-01-01 00:10:46|  2021-01-01 00:32:58|              2|         7.40|         1|                 N|         138|         166|           2|       24.5|  2.5|    0.5|         0|        6.12|                  0.3|       33.92|                   0|\n",
      "|       2| 2021-01-01 00:31:06|  2021-01-01 00:38:52|              5|         1.70|         1|                 N|         142|          50|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2021-01-01 00:42:11|  2021-01-01 00:44:24|              5|          .81|         1|                 N|          50|         142|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2021-01-01 00:17:48|  2021-01-01 00:21:55|              1|         1.01|         1|                 N|         236|         237|           1|        5.5|  0.5|    0.5|         1|           0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-01 00:33:38|  2021-01-01 00:38:37|              1|          .73|         1|                 N|         142|         239|           1|        5.5|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                 2.5|\n",
      "|       2| 2021-01-01 00:47:56|  2021-01-01 00:52:53|              1|         1.17|         1|                 N|         238|         166|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|\n",
      "|       2| 2021-01-01 00:04:21|  2021-01-01 00:07:58|              1|          .78|         1|                 N|         239|         238|           1|        4.5|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|                 2.5|\n",
      "|       2| 2021-01-01 00:18:36|  2021-01-01 00:27:10|              2|         1.66|         1|                 N|         151|         142|           2|        8.5|  0.5|    0.5|         0|           0|                  0.3|        12.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:27:12,124 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1424 ms on 172.22.0.5 (executor 0) (1/1)\n",
      "2023-12-17 09:27:12,124 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:27:12,126 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.444 s\n",
      "2023-12-17 09:27:12,127 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:27:12,127 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-12-17 09:27:12,127 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.449946 s\n",
      "2023-12-17 09:27:12,191 INFO codegen.CodeGenerator: Code generated in 45.373005 ms\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb8f687-4c8f-4bcd-8a5c-baf2e00574db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:27:20,798 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:27:20,798 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:27:20,799 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:27:21,746 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:27:21,768 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-17 09:27:21,768 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-17 09:27:21,769 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:27:21,770 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-12-17 09:27:21,770 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-12-17 09:27:21,771 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-12-17 09:27:22,875 INFO gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=1103; previousMaxLatencyMs=0; operationCount=1; context=gs://datalake-playground/yellow_tripdata/2021/01/_temporary/0\n",
      "2023-12-17 09:27:22,879 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 304.0 KiB, free 433.0 MiB)\n",
      "2023-12-17 09:27:22,894 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 53.8 KiB, free 433.0 MiB)\n",
      "2023-12-17 09:27:22,896 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on a31eab60218a:42037 (size: 53.8 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:27:22,897 INFO spark.SparkContext: Created broadcast 5 from parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:22,899 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 65087833 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:27:22,936 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:22,938 INFO scheduler.DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-12-17 09:27:22,938 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:27:22,938 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:27:22,938 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:27:22,939 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:27:22,961 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 315.6 KiB, free 432.7 MiB)\n",
      "2023-12-17 09:27:22,965 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 115.7 KiB, free 432.6 MiB)\n",
      "2023-12-17 09:27:22,967 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on a31eab60218a:42037 (size: 115.7 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:27:22,968 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:27:22,969 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-12-17 09:27:22,969 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "2023-12-17 09:27:22,970 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.22.0.5, executor 0, partition 0, PROCESS_LOCAL, 4927 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:27:22,971 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.22.0.6, executor 1, partition 1, PROCESS_LOCAL, 4927 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:27:22,995 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.5:46397 (size: 115.7 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:27:23,275 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.22.0.6:45463 (size: 115.7 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:24,841 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.5:46397 (size: 53.8 KiB, free: 126.9 MiB)\n",
      "2023-12-17 09:27:25,680 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.22.0.6:45463 (size: 53.8 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:27:45,177 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 22207 ms on 172.22.0.5 (executor 0) (1/2)\n",
      "2023-12-17 09:27:47,336 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 24366 ms on 172.22.0.6 (executor 1) (2/2)\n",
      "2023-12-17 09:27:47,336 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:27:47,338 INFO scheduler.DAGScheduler: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 24.396 s\n",
      "2023-12-17 09:27:47,338 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:27:47,338 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "2023-12-17 09:27:47,339 INFO scheduler.DAGScheduler: Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 24.402098 s\n",
      "2023-12-17 09:27:47,341 INFO datasources.FileFormatWriter: Start to commit write Job ee718623-f0d4-47ba-8038-65ffaeefd808.\n",
      "2023-12-17 09:27:50,922 INFO gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://datalake-playground/yellow_tripdata/2021/01/_temporary/0/task_202312170927228522255724857901825_0002_m_000001/' directory.\n",
      "2023-12-17 09:27:50,923 INFO gcs.GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=1794; previousMaxLatencyMs=0; operationCount=1; context=rename(gs://datalake-playground/yellow_tripdata/2021/01/_temporary/0/task_202312170927228522255724857901825_0002_m_000001/part-00001-31cfa277-3810-46dd-bb7b-beb30f8e18ac-c000.snappy.parquet -> gs://datalake-playground/yellow_tripdata/2021/01/part-00001-31cfa277-3810-46dd-bb7b-beb30f8e18ac-c000.snappy.parquet)\n",
      "2023-12-17 09:27:53,815 INFO gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://datalake-playground/yellow_tripdata/2021/01/_temporary/0/task_202312170927228664015530472174153_0002_m_000000/' directory.\n",
      "2023-12-17 09:27:55,642 INFO gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://datalake-playground/yellow_tripdata/2021/01/' directory.\n",
      "2023-12-17 09:27:55,643 INFO gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=1825; previousMaxLatencyMs=0; operationCount=1; context=gs://datalake-playground/yellow_tripdata/2021/01/_temporary\n",
      "2023-12-17 09:27:56,609 INFO gcs.GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=961; previousMaxLatencyMs=0; operationCount=1; context=gs://datalake-playground/yellow_tripdata/2021/01/_SUCCESS\n",
      "2023-12-17 09:27:56,611 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on a31eab60218a:42037 in memory (size: 53.7 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:27:56,622 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on a31eab60218a:42037 in memory (size: 6.2 KiB, free: 434.1 MiB)\n",
      "2023-12-17 09:27:56,626 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 172.22.0.5:46397 in memory (size: 6.2 KiB, free: 126.9 MiB)\n",
      "2023-12-17 09:27:56,658 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on a31eab60218a:42037 in memory (size: 115.7 KiB, free: 434.2 MiB)\n",
      "2023-12-17 09:27:56,660 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.5:46397 in memory (size: 115.7 KiB, free: 127.0 MiB)\n",
      "2023-12-17 09:27:56,669 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 172.22.0.6:45463 in memory (size: 115.7 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:56,680 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on a31eab60218a:42037 in memory (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:56,683 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 172.22.0.5:46397 in memory (size: 53.8 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:56,693 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on a31eab60218a:42037 in memory (size: 53.7 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:56,696 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 172.22.0.5:46397 in memory (size: 53.7 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:27:57,590 INFO gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=980; previousMaxLatencyMs=0; operationCount=1; context=gs://datalake-playground/yellow_tripdata/2021/01/_SUCCESS\n",
      "2023-12-17 09:27:58,130 INFO datasources.FileFormatWriter: Write Job ee718623-f0d4-47ba-8038-65ffaeefd808 committed. Elapsed time: 10787 ms.\n",
      "2023-12-17 09:27:58,134 INFO datasources.FileFormatWriter: Finished processing stats for write job ee718623-f0d4-47ba-8038-65ffaeefd808.\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet(f\"gs://{os.environ['GOOGLE_STORAGE_BUCKET']}/yellow_tripdata/2021/01/\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c37d24-bd29-432e-a449-07d6910c1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:27:59,868 INFO datasources.InMemoryFileIndex: It took 554 ms to list leaf files for 1 paths.\n",
      "2023-12-17 09:27:59,935 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:27:59,936 INFO scheduler.DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:27:59,936 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:27:59,936 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:27:59,937 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:27:59,938 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:27:59,957 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 153.3 KiB, free 433.9 MiB)\n",
      "2023-12-17 09:27:59,967 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 56.1 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:27:59,969 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on a31eab60218a:42037 (size: 56.1 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:59,970 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on a31eab60218a:42037 in memory (size: 53.8 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:27:59,970 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:27:59,971 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:27:59,972 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:27:59,973 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.5:46397 in memory (size: 53.8 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:27:59,976 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 172.22.0.6:45463 in memory (size: 53.8 KiB, free: 127.2 MiB)\n",
      "2023-12-17 09:27:59,976 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4668 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:28:00,042 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.22.0.6:45463 (size: 56.1 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:28:02,317 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 2344 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:28:02,317 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:28:02,318 INFO scheduler.DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.378 s\n",
      "2023-12-17 09:28:02,318 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:28:02,318 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "2023-12-17 09:28:02,319 INFO scheduler.DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.383461 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(f\"gs://{os.environ['GOOGLE_STORAGE_BUCKET']}/yellow_tripdata/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5847c287-629c-43d2-8b5e-3dfb2763da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:28:02,386 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-12-17 09:28:02,386 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-12-17 09:28:02,386 INFO datasources.FileSourceStrategy: Output Data Schema: struct<VendorID: string, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string ... 16 more fields>\n",
      "2023-12-17 09:28:02,459 INFO codegen.CodeGenerator: Code generated in 40.494551 ms\n",
      "2023-12-17 09:28:02,469 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 307.8 KiB, free 433.9 MiB)\n",
      "2023-12-17 09:28:02,480 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 54.3 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:28:02,482 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on a31eab60218a:42037 (size: 54.3 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:28:02,483 INFO spark.SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:28:02,487 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18086330 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-12-17 09:28:02,501 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-12-17 09:28:02,503 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-12-17 09:28:02,503 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-12-17 09:28:02,503 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-12-17 09:28:02,504 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-12-17 09:28:02,505 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-12-17 09:28:02,527 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 22.8 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:28:02,530 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 433.8 MiB)\n",
      "2023-12-17 09:28:02,531 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on a31eab60218a:42037 (size: 7.1 KiB, free: 434.3 MiB)\n",
      "2023-12-17 09:28:02,532 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "2023-12-17 09:28:02,533 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-12-17 09:28:02,533 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-12-17 09:28:02,535 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.22.0.6, executor 1, partition 0, PROCESS_LOCAL, 4991 bytes) taskResourceAssignments Map()\n",
      "2023-12-17 09:28:02,559 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.22.0.6:45463 (size: 7.1 KiB, free: 127.1 MiB)\n",
      "2023-12-17 09:28:02,691 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.22.0.6:45463 (size: 54.3 KiB, free: 127.1 MiB)\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2021-01-01 00:30:10|  2021-01-01 00:36:12|              1|         2.10|         1|                 N|         142|          43|           2|          8|    3|    0.5|         0|           0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2021-01-01 00:51:20|  2021-01-01 00:52:19|              1|          .20|         1|                 N|         238|         151|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|                   0|\n",
      "|       1| 2021-01-01 00:43:30|  2021-01-01 01:11:06|              1|        14.70|         1|                 N|         132|         165|           1|         42|  0.5|    0.5|      8.65|           0|                  0.3|       51.95|                   0|\n",
      "|       1| 2021-01-01 00:15:48|  2021-01-01 00:31:01|              0|        10.60|         1|                 N|         138|         132|           1|         29|  0.5|    0.5|      6.05|           0|                  0.3|       36.35|                   0|\n",
      "|       2| 2021-01-01 00:31:49|  2021-01-01 00:48:21|              1|         4.94|         1|                 N|          68|          33|           1|       16.5|  0.5|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|\n",
      "|       1| 2021-01-01 00:16:29|  2021-01-01 00:24:30|              1|         1.60|         1|                 N|         224|          68|           1|          8|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       1| 2021-01-01 00:00:28|  2021-01-01 00:17:28|              1|         4.10|         1|                 N|          95|         157|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        17.3|                   0|\n",
      "|       1| 2021-01-01 00:12:29|  2021-01-01 00:30:34|              1|         5.70|         1|                 N|          90|          40|           2|         18|    3|    0.5|         0|           0|                  0.3|        21.8|                 2.5|\n",
      "|       1| 2021-01-01 00:39:16|  2021-01-01 01:00:13|              1|         9.10|         1|                 N|          97|         129|           4|       27.5|  0.5|    0.5|         0|           0|                  0.3|        28.8|                   0|\n",
      "|       1| 2021-01-01 00:26:12|  2021-01-01 00:39:46|              2|         2.70|         1|                 N|         263|         142|           1|         12|    3|    0.5|      3.15|           0|                  0.3|       18.95|                 2.5|\n",
      "|       2| 2021-01-01 00:15:52|  2021-01-01 00:38:07|              3|         6.11|         1|                 N|         164|         255|           1|       20.5|  0.5|    0.5|         0|           0|                  0.3|        24.3|                 2.5|\n",
      "|       2| 2021-01-01 00:46:36|  2021-01-01 00:53:45|              2|         1.21|         1|                 N|         255|          80|           1|          7|  0.5|    0.5|      2.49|           0|                  0.3|       10.79|                   0|\n",
      "|       1| 2021-01-01 00:10:46|  2021-01-01 00:32:58|              2|         7.40|         1|                 N|         138|         166|           2|       24.5|  2.5|    0.5|         0|        6.12|                  0.3|       33.92|                   0|\n",
      "|       2| 2021-01-01 00:31:06|  2021-01-01 00:38:52|              5|         1.70|         1|                 N|         142|          50|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2021-01-01 00:42:11|  2021-01-01 00:44:24|              5|          .81|         1|                 N|          50|         142|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2021-01-01 00:17:48|  2021-01-01 00:21:55|              1|         1.01|         1|                 N|         236|         237|           1|        5.5|  0.5|    0.5|         1|           0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-01 00:33:38|  2021-01-01 00:38:37|              1|          .73|         1|                 N|         142|         239|           1|        5.5|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                 2.5|\n",
      "|       2| 2021-01-01 00:47:56|  2021-01-01 00:52:53|              1|         1.17|         1|                 N|         238|         166|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|\n",
      "|       2| 2021-01-01 00:04:21|  2021-01-01 00:07:58|              1|          .78|         1|                 N|         239|         238|           1|        4.5|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|                 2.5|\n",
      "|       2| 2021-01-01 00:18:36|  2021-01-01 00:27:10|              2|         1.66|         1|                 N|         151|         142|           2|        8.5|  0.5|    0.5|         0|           0|                  0.3|        12.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:28:10,555 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 8021 ms on 172.22.0.6 (executor 1) (1/1)\n",
      "2023-12-17 09:28:10,555 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-12-17 09:28:10,556 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 8.050 s\n",
      "2023-12-17 09:28:10,556 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-12-17 09:28:10,556 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2023-12-17 09:28:10,557 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 8.055501 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33bbce25-e2e8-46d9-8abb-dfc2fde658f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 09:28:10,604 INFO server.AbstractConnector: Stopped Spark@411e682a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-12-17 09:28:10,609 INFO ui.SparkUI: Stopped Spark web UI at http://a31eab60218a:4040\n",
      "2023-12-17 09:28:10,615 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors\n",
      "2023-12-17 09:28:10,616 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "2023-12-17 09:28:11,040 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-12-17 09:28:11,056 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-12-17 09:28:11,057 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-12-17 09:28:11,060 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-12-17 09:28:11,064 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-12-17 09:28:11,078 INFO spark.SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
